{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a2a782",
   "metadata": {},
   "source": [
    "# Fine-tuning and Custom Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers fine-tuning techniques for creating custom language models and embeddings. We'll explore:\n",
    "\n",
    "1. **Fine-tuning Fundamentals**: Understanding when and why to fine-tune\n",
    "2. **Data Preparation**: Creating high-quality training datasets\n",
    "3. **Fine-tuning Techniques**: LoRA, QLoRA, and full fine-tuning\n",
    "4. **Custom Embeddings**: Training domain-specific embedding models\n",
    "5. **Evaluation and Validation**: Measuring fine-tuned model performance\n",
    "6. **Production Deployment**: Serving custom models efficiently\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand different fine-tuning approaches and their trade-offs\n",
    "- Know how to prepare and validate training data\n",
    "- Be able to fine-tune models using various techniques\n",
    "- Create custom embedding models for specific domains\n",
    "- Evaluate and deploy fine-tuned models effectively\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Fine-tuning Fundamentals](#fundamentals)\n",
    "2. [Data Preparation](#data-prep)\n",
    "3. [Fine-tuning Techniques](#techniques)\n",
    "4. [Custom Embeddings](#embeddings)\n",
    "5. [Evaluation and Validation](#evaluation)\n",
    "6. [Production Deployment](#deployment)\n",
    "7. [Real-world Applications](#applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931641aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes torch torchvision torchaudio sentence-transformers scikit-learn matplotlib seaborn wandb\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import bitsandbytes as bnb\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our LLM provider utilities\n",
    "import sys\n",
    "sys.path.append('../../utils')\n",
    "from llm_providers import get_available_providers, LLMProviderFactory, LLMConfig\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM provider\n",
    "available_providers = get_available_providers()\n",
    "if not available_providers:\n",
    "    raise ValueError(\"No LLM providers available! Please check your .env file and API keys.\")\n",
    "\n",
    "provider_name = list(available_providers.keys())[0]\n",
    "llm_provider = available_providers[provider_name]\n",
    "\n",
    "print(f\"‚úÖ All packages imported successfully!\")\n",
    "print(f\"üîß Using LLM provider: {provider_name.upper()}\")\n",
    "print(f\"üåê Available providers: {list(available_providers.keys())}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"üîß Environment configured for fine-tuning and custom models\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
