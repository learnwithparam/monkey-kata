{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f1ffa9",
   "metadata": {},
   "source": [
    "# Document Splitting Techniques: The Foundation of Effective RAG\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Document Splitting](#introduction)\n",
    "2. [Fixed-Size Chunking](#fixed-size)\n",
    "3. [Semantic Chunking](#semantic-chunking)\n",
    "4. [Recursive Character Splitting](#recursive)\n",
    "5. [Sentence-Aware Splitting](#sentence-aware)\n",
    "6. [HTML/Markdown Splitting](#html-markdown)\n",
    "7. [Code Splitting](#code-splitting)\n",
    "8. [Performance Comparison](#performance)\n",
    "9. [Best Practices & Guidelines](#best-practices)\n",
    "10. [Real-World Examples](#real-world)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Document Splitting {#introduction}\n",
    "\n",
    "Document splitting is the process of breaking down large documents into smaller, manageable chunks that can be effectively processed by embedding models and retrieved by RAG systems.\n",
    "\n",
    "### Why Splitting Matters\n",
    "\n",
    "**The Challenge**: \n",
    "- LLMs have context limits (e.g., 4K, 8K, 32K tokens)\n",
    "- Large documents can overwhelm the context window\n",
    "- Irrelevant information can dilute the response quality\n",
    "\n",
    "**The Solution**:\n",
    "- Split documents into focused, coherent chunks\n",
    "- Preserve semantic meaning within chunks\n",
    "- Enable precise retrieval of relevant information\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "1. **Chunk Size**: Balance between context and precision\n",
    "2. **Overlap**: Ensure continuity between chunks\n",
    "3. **Semantic Boundaries**: Respect natural language boundaries\n",
    "4. **Metadata Preservation**: Maintain document structure and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-text-splitters tiktoken nltk spacy sentence-transformers\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set up encoding for token counting\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")\n",
    "print(\"ðŸ”§ Environment configured for document splitting analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d55be3",
   "metadata": {},
   "source": [
    "## Fixed-Size Chunking {#fixed-size}\n",
    "\n",
    "The simplest and most common approach - split text into chunks of fixed character or token length.\n",
    "\n",
    "### Pros:\n",
    "- âœ… Simple to implement\n",
    "- âœ… Predictable chunk sizes\n",
    "- âœ… Fast processing\n",
    "- âœ… Works with any text type\n",
    "\n",
    "### Cons:\n",
    "- âŒ May break sentences mid-way\n",
    "- âŒ Can lose semantic context\n",
    "- âŒ No consideration of content structure\n",
    "- âŒ May create incoherent chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4734492",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Represents a text chunk with metadata\"\"\"\n",
    "    content: str\n",
    "    start_index: int\n",
    "    end_index: int\n",
    "    chunk_id: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class FixedSizeSplitter:\n",
    "    \"\"\"Fixed-size text splitter\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50, \n",
    "                 use_tokens: bool = False):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.use_tokens = use_tokens\n",
    "        \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    def split_text(self, text: str, metadata: Dict = None) -> List[Chunk]:\n",
    "        \"\"\"Split text into fixed-size chunks\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        if self.use_tokens:\n",
    "            # Token-based splitting\n",
    "            tokens = encoding.encode(text)\n",
    "            start = 0\n",
    "            chunk_id = 0\n",
    "            \n",
    "            while start < len(tokens):\n",
    "                end = min(start + self.chunk_size, len(tokens))\n",
    "                chunk_tokens = tokens[start:end]\n",
    "                chunk_text = encoding.decode(chunk_tokens)\n",
    "                \n",
    "                chunk = Chunk(\n",
    "                    content=chunk_text,\n",
    "                    start_index=start,\n",
    "                    end_index=end,\n",
    "                    chunk_id=f\"chunk_{chunk_id}\",\n",
    "                    metadata=metadata or {}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                start = end - self.chunk_overlap\n",
    "                chunk_id += 1\n",
    "                \n",
    "                if start >= len(tokens) - self.chunk_overlap:\n",
    "                    break\n",
    "        else:\n",
    "            # Character-based splitting\n",
    "            start = 0\n",
    "            chunk_id = 0\n",
    "            \n",
    "            while start < len(text):\n",
    "                end = min(start + self.chunk_size, len(text))\n",
    "                chunk_text = text[start:end]\n",
    "                \n",
    "                chunk = Chunk(\n",
    "                    content=chunk_text,\n",
    "                    start_index=start,\n",
    "                    end_index=end,\n",
    "                    chunk_id=f\"chunk_{chunk_id}\",\n",
    "                    metadata=metadata or {}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                start = end - self.chunk_overlap\n",
    "                chunk_id += 1\n",
    "                \n",
    "                if start >= len(text) - self.chunk_overlap:\n",
    "                    break\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Sample technical documentation\n",
    "sample_doc = \"\"\"\n",
    "# Machine Learning Fundamentals\n",
    "\n",
    "## Introduction\n",
    "Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data. It has revolutionized many industries including healthcare, finance, and technology.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning uses labeled training data to learn a mapping from inputs to outputs. Common algorithms include:\n",
    "- Linear Regression: Used for predicting continuous values\n",
    "- Decision Trees: Create a tree-like model of decisions\n",
    "- Random Forest: Ensemble method using multiple decision trees\n",
    "- Support Vector Machines: Find optimal hyperplane for classification\n",
    "\n",
    "### Unsupervised Learning\n",
    "Unsupervised learning finds patterns in data without labeled examples:\n",
    "- Clustering: Group similar data points together\n",
    "- Dimensionality Reduction: Reduce number of features while preserving information\n",
    "- Association Rules: Find relationships between variables\n",
    "\n",
    "### Reinforcement Learning\n",
    "Reinforcement learning learns through interaction with an environment:\n",
    "- Agent takes actions in an environment\n",
    "- Receives rewards or penalties\n",
    "- Learns optimal policy through trial and error\n",
    "\n",
    "## Applications\n",
    "Machine learning is used in:\n",
    "- Recommendation systems (Netflix, Amazon)\n",
    "- Image recognition (medical diagnosis, autonomous vehicles)\n",
    "- Natural language processing (chatbots, translation)\n",
    "- Fraud detection (banking, insurance)\n",
    "- Predictive analytics (weather, stock markets)\n",
    "\n",
    "## Best Practices\n",
    "1. Start with simple models\n",
    "2. Ensure data quality\n",
    "3. Use cross-validation\n",
    "4. Monitor for overfitting\n",
    "5. Consider interpretability\n",
    "\"\"\"\n",
    "\n",
    "# Test fixed-size splitting\n",
    "print(\"ðŸ“„ Sample Document Length:\", len(sample_doc), \"characters\")\n",
    "print(\"ðŸ”¢ Token Count:\", len(encoding.encode(sample_doc)), \"tokens\")\n",
    "\n",
    "# Character-based splitting\n",
    "char_splitter = FixedSizeSplitter(chunk_size=200, chunk_overlap=50, use_tokens=False)\n",
    "char_chunks = char_splitter.split_text(sample_doc, {\"source\": \"ml_fundamentals\"})\n",
    "\n",
    "print(f\"\\nðŸ“Š Character-based splitting results:\")\n",
    "print(f\"Number of chunks: {len(char_chunks)}\")\n",
    "print(f\"Average chunk length: {np.mean([len(chunk.content) for chunk in char_chunks]):.0f} characters\")\n",
    "\n",
    "# Token-based splitting\n",
    "token_splitter = FixedSizeSplitter(chunk_size=100, chunk_overlap=25, use_tokens=True)\n",
    "token_chunks = token_splitter.split_text(sample_doc, {\"source\": \"ml_fundamentals\"})\n",
    "\n",
    "print(f\"\\nðŸ“Š Token-based splitting results:\")\n",
    "print(f\"Number of chunks: {len(token_chunks)}\")\n",
    "print(f\"Average token count: {np.mean([len(encoding.encode(chunk.content)) for chunk in token_chunks]):.0f} tokens\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nðŸ” Sample chunks (character-based):\")\n",
    "for i, chunk in enumerate(char_chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1} (ID: {chunk.chunk_id}):\")\n",
    "    print(f\"Content: {chunk.content[:150]}...\")\n",
    "    print(f\"Length: {len(chunk.content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc4b01",
   "metadata": {},
   "source": [
    "## Semantic Chunking {#semantic-chunking}\n",
    "\n",
    "Split text based on semantic similarity - group sentences that are semantically related together.\n",
    "\n",
    "### Pros:\n",
    "- âœ… Preserves semantic coherence\n",
    "- âœ… Better for retrieval tasks\n",
    "- âœ… Reduces noise in chunks\n",
    "- âœ… More meaningful chunk boundaries\n",
    "\n",
    "### Cons:\n",
    "- âŒ More complex to implement\n",
    "- âŒ Requires embedding model\n",
    "- âŒ Slower processing\n",
    "- âŒ May create very small or large chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSplitter:\n",
    "    \"\"\"Semantic text splitter based on sentence similarity\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", \n",
    "                 similarity_threshold: float = 0.7,\n",
    "                 min_chunk_size: int = 100,\n",
    "                 max_chunk_size: int = 1000):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        \n",
    "    def split_text(self, text: str, metadata: Dict = None) -> List[Chunk]:\n",
    "        \"\"\"Split text based on semantic similarity\"\"\"\n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        if len(sentences) <= 1:\n",
    "            return [Chunk(\n",
    "                content=text,\n",
    "                start_index=0,\n",
    "                end_index=len(text),\n",
    "                chunk_id=\"chunk_0\",\n",
    "                metadata=metadata or {}\n",
    "            )]\n",
    "        \n",
    "        # Generate embeddings for sentences\n",
    "        sentence_embeddings = self.model.encode(sentences)\n",
    "        \n",
    "        # Group sentences by similarity\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        current_start = 0\n",
    "        \n",
    "        for i in range(1, len(sentences)):\n",
    "            # Calculate similarity with previous sentence\n",
    "            similarity = np.dot(sentence_embeddings[i-1], sentence_embeddings[i])\n",
    "            \n",
    "            # Check if we should start a new chunk\n",
    "            current_text = \" \".join(current_chunk)\n",
    "            should_split = (\n",
    "                similarity < self.similarity_threshold or\n",
    "                len(current_text) > self.max_chunk_size\n",
    "            )\n",
    "            \n",
    "            if should_split and len(current_text) >= self.min_chunk_size:\n",
    "                # Create chunk\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                chunk = Chunk(\n",
    "                    content=chunk_text,\n",
    "                    start_index=current_start,\n",
    "                    end_index=current_start + len(chunk_text),\n",
    "                    chunk_id=f\"chunk_{len(chunks)}\",\n",
    "                    metadata=metadata or {}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                # Start new chunk\n",
    "                current_chunk = [sentences[i]]\n",
    "                current_start += len(chunk_text) + 1\n",
    "            else:\n",
    "                current_chunk.append(sentences[i])\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                start_index=current_start,\n",
    "                end_index=current_start + len(chunk_text),\n",
    "                chunk_id=f\"chunk_{len(chunks)}\",\n",
    "                metadata=metadata or {}\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test semantic splitting\n",
    "semantic_splitter = SemanticSplitter(\n",
    "    similarity_threshold=0.6,\n",
    "    min_chunk_size=150,\n",
    "    max_chunk_size=800\n",
    ")\n",
    "\n",
    "semantic_chunks = semantic_splitter.split_text(sample_doc, {\"source\": \"ml_fundamentals\"})\n",
    "\n",
    "print(f\"ðŸ“Š Semantic splitting results:\")\n",
    "print(f\"Number of chunks: {len(semantic_chunks)}\")\n",
    "print(f\"Average chunk length: {np.mean([len(chunk.content) for chunk in semantic_chunks]):.0f} characters\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nðŸ” Sample semantic chunks:\")\n",
    "for i, chunk in enumerate(semantic_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (ID: {chunk.chunk_id}):\")\n",
    "    print(f\"Content: {chunk.content[:200]}...\")\n",
    "    print(f\"Length: {len(chunk.content)} characters\")\n",
    "    print(f\"Sentences: {len(sent_tokenize(chunk.content))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2ee47",
   "metadata": {},
   "source": [
    "## Recursive Character Splitting {#recursive}\n",
    "\n",
    "A hierarchical approach that tries different splitting strategies in order of preference.\n",
    "\n",
    "### Pros:\n",
    "- âœ… Respects document structure\n",
    "- âœ… Handles multiple text types\n",
    "- âœ… Configurable splitting hierarchy\n",
    "- âœ… Good balance of structure and size\n",
    "\n",
    "### Cons:\n",
    "- âŒ Can be complex to configure\n",
    "- âŒ May not always find optimal splits\n",
    "- âŒ Requires understanding of document structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveSplitter:\n",
    "    \"\"\"Recursive character splitter with multiple strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Define splitting separators in order of preference\n",
    "        self.separators = [\n",
    "            \"\\n\\n\",  # Paragraph breaks\n",
    "            \"\\n\",    # Line breaks\n",
    "            \". \",    # Sentence endings\n",
    "            \"! \",    # Exclamation marks\n",
    "            \"? \",    # Question marks\n",
    "            \"; \",    # Semicolons\n",
    "            \", \",    # Commas\n",
    "            \" \",     # Spaces\n",
    "            \"\"       # Character level\n",
    "        ]\n",
    "    \n",
    "    def split_text(self, text: str, metadata: Dict = None) -> List[Chunk]:\n",
    "        \"\"\"Split text using recursive strategy\"\"\"\n",
    "        return self._split_text_recursive(text, 0, metadata or {})\n",
    "    \n",
    "    def _split_text_recursive(self, text: str, start_index: int, \n",
    "                            metadata: Dict, separator_index: int = 0) -> List[Chunk]:\n",
    "        \"\"\"Recursively split text using separators\"\"\"\n",
    "        \n",
    "        # If text is small enough, return as single chunk\n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [Chunk(\n",
    "                content=text,\n",
    "                start_index=start_index,\n",
    "                end_index=start_index + len(text),\n",
    "                chunk_id=f\"chunk_{start_index}\",\n",
    "                metadata=metadata\n",
    "            )]\n",
    "        \n",
    "        # If we've tried all separators, split by character\n",
    "        if separator_index >= len(self.separators):\n",
    "            return self._split_by_character(text, start_index, metadata)\n",
    "        \n",
    "        separator = self.separators[separator_index]\n",
    "        \n",
    "        # Split by current separator\n",
    "        if separator:\n",
    "            parts = text.split(separator)\n",
    "        else:\n",
    "            parts = list(text)\n",
    "        \n",
    "        # If splitting didn't help, try next separator\n",
    "        if len(parts) <= 1:\n",
    "            return self._split_text_recursive(text, start_index, metadata, separator_index + 1)\n",
    "        \n",
    "        # Group parts into chunks\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_start = start_index\n",
    "        \n",
    "        for i, part in enumerate(parts):\n",
    "            # Add separator back (except for last part)\n",
    "            if i < len(parts) - 1 and separator:\n",
    "                part_with_sep = part + separator\n",
    "            else:\n",
    "                part_with_sep = part\n",
    "            \n",
    "            # Check if adding this part would exceed chunk size\n",
    "            if len(current_chunk + part_with_sep) > self.chunk_size and current_chunk:\n",
    "                # Create chunk\n",
    "                chunk = Chunk(\n",
    "                    content=current_chunk.strip(),\n",
    "                    start_index=current_start,\n",
    "                    end_index=current_start + len(current_chunk),\n",
    "                    chunk_id=f\"chunk_{current_start}\",\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_text = current_chunk[-self.chunk_overlap:] if self.chunk_overlap > 0 else \"\"\n",
    "                current_chunk = overlap_text + part_with_sep\n",
    "                current_start += len(current_chunk) - len(overlap_text)\n",
    "            else:\n",
    "                current_chunk += part_with_sep\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunk = Chunk(\n",
    "                content=current_chunk.strip(),\n",
    "                start_index=current_start,\n",
    "                end_index=current_start + len(current_chunk),\n",
    "                chunk_id=f\"chunk_{current_start}\",\n",
    "                metadata=metadata\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # If chunks are still too large, try next separator\n",
    "        if any(len(chunk.content) > self.chunk_size for chunk in chunks):\n",
    "            return self._split_text_recursive(text, start_index, metadata, separator_index + 1)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_by_character(self, text: str, start_index: int, metadata: Dict) -> List[Chunk]:\n",
    "        \"\"\"Split text by character when all other methods fail\"\"\"\n",
    "        chunks = []\n",
    "        current_start = start_index\n",
    "        \n",
    "        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):\n",
    "            end = min(i + self.chunk_size, len(text))\n",
    "            chunk_text = text[i:end]\n",
    "            \n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                start_index=current_start,\n",
    "                end_index=current_start + len(chunk_text),\n",
    "                chunk_id=f\"chunk_{current_start}\",\n",
    "                metadata=metadata\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "            current_start += len(chunk_text) - self.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test recursive splitting\n",
    "recursive_splitter = RecursiveSplitter(chunk_size=300, chunk_overlap=50)\n",
    "recursive_chunks = recursive_splitter.split_text(sample_doc, {\"source\": \"ml_fundamentals\"})\n",
    "\n",
    "print(f\"ðŸ“Š Recursive splitting results:\")\n",
    "print(f\"Number of chunks: {len(recursive_chunks)}\")\n",
    "print(f\"Average chunk length: {np.mean([len(chunk.content) for chunk in recursive_chunks]):.0f} characters\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nðŸ” Sample recursive chunks:\")\n",
    "for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (ID: {chunk.chunk_id}):\")\n",
    "    print(f\"Content: {chunk.content[:200]}...\")\n",
    "    print(f\"Length: {len(chunk.content)} characters\")\n",
    "    print(f\"Starts with: '{chunk.content[:50]}...'\")\n",
    "    print(f\"Ends with: '...{chunk.content[-50:]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17943671",
   "metadata": {},
   "source": [
    "## Sentence-Aware Splitting {#sentence-aware}\n",
    "\n",
    "Split text while respecting sentence boundaries to maintain coherence.\n",
    "\n",
    "### Pros:\n",
    "- âœ… Preserves sentence integrity\n",
    "- âœ… Better for natural language processing\n",
    "- âœ… Maintains grammatical structure\n",
    "- âœ… Good for question-answering tasks\n",
    "\n",
    "### Cons:\n",
    "- âŒ May create very small chunks\n",
    "- âŒ Doesn't consider semantic similarity\n",
    "- âŒ Can break paragraph context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAwareSplitter:\n",
    "    \"\"\"Split text while respecting sentence boundaries\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def split_text(self, text: str, metadata: Dict = None) -> List[Chunk]:\n",
    "        \"\"\"Split text respecting sentence boundaries\"\"\"\n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        if len(sentences) <= 1:\n",
    "            return [Chunk(\n",
    "                content=text,\n",
    "                start_index=0,\n",
    "                end_index=len(text),\n",
    "                chunk_id=\"chunk_0\",\n",
    "                metadata=metadata or {}\n",
    "            )]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        current_start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            # Check if adding this sentence would exceed chunk size\n",
    "            if current_length + sentence_length > self.chunk_size and current_chunk:\n",
    "                # Create chunk\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                chunk = Chunk(\n",
    "                    content=chunk_text,\n",
    "                    start_index=current_start,\n",
    "                    end_index=current_start + len(chunk_text),\n",
    "                    chunk_id=f\"chunk_{chunk_id}\",\n",
    "                    metadata=metadata or {}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                if self.chunk_overlap > 0:\n",
    "                    # Keep last few sentences for overlap\n",
    "                    overlap_sentences = []\n",
    "                    overlap_length = 0\n",
    "                    for sent in reversed(current_chunk):\n",
    "                        if overlap_length + len(sent) <= self.chunk_overlap:\n",
    "                            overlap_sentences.insert(0, sent)\n",
    "                            overlap_length += len(sent)\n",
    "                        else:\n",
    "                            break\n",
    "                    current_chunk = overlap_sentences + [sentence]\n",
    "                    current_length = overlap_length + sentence_length\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_length = sentence_length\n",
    "                \n",
    "                current_start += len(chunk_text) - overlap_length if self.chunk_overlap > 0 else len(chunk_text)\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                start_index=current_start,\n",
    "                end_index=current_start + len(chunk_text),\n",
    "                chunk_id=f\"chunk_{chunk_id}\",\n",
    "                metadata=metadata or {}\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test sentence-aware splitting\n",
    "sentence_splitter = SentenceAwareSplitter(chunk_size=400, chunk_overlap=100)\n",
    "sentence_chunks = sentence_splitter.split_text(sample_doc, {\"source\": \"ml_fundamentals\"})\n",
    "\n",
    "print(f\"ðŸ“Š Sentence-aware splitting results:\")\n",
    "print(f\"Number of chunks: {len(sentence_chunks)}\")\n",
    "print(f\"Average chunk length: {np.mean([len(chunk.content) for chunk in sentence_chunks]):.0f} characters\")\n",
    "print(f\"Average sentences per chunk: {np.mean([len(sent_tokenize(chunk.content)) for chunk in sentence_chunks]):.1f}\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nðŸ” Sample sentence-aware chunks:\")\n",
    "for i, chunk in enumerate(sentence_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (ID: {chunk.chunk_id}):\")\n",
    "    print(f\"Content: {chunk.content[:200]}...\")\n",
    "    print(f\"Length: {len(chunk.content)} characters\")\n",
    "    print(f\"Sentences: {len(sent_tokenize(chunk.content))}\")\n",
    "    print(f\"First sentence: '{sent_tokenize(chunk.content)[0][:100]}...'\")\n",
    "    print(f\"Last sentence: '...{sent_tokenize(chunk.content)[-1][-100:]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7c563",
   "metadata": {},
   "source": [
    "## HTML/Markdown Splitting {#html-markdown}\n",
    "\n",
    "Specialized splitting for structured documents like HTML and Markdown that preserves document hierarchy.\n",
    "\n",
    "### Pros:\n",
    "- âœ… Preserves document structure\n",
    "- âœ… Maintains heading hierarchy\n",
    "- âœ… Good for technical documentation\n",
    "- âœ… Enables section-based retrieval\n",
    "\n",
    "### Cons:\n",
    "- âŒ Requires parsing HTML/Markdown\n",
    "- âŒ May create very small chunks\n",
    "- âŒ Complex to implement correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e03558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkdownSplitter:\n",
    "    \"\"\"Split Markdown documents while preserving structure\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def split_text(self, text: str, metadata: Dict = None) -> List[Chunk]:\n",
    "        \"\"\"Split Markdown text preserving structure\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        # Split by headers first\n",
    "        sections = self._split_by_headers(text)\n",
    "        \n",
    "        for section in sections:\n",
    "            section_text, section_metadata = section\n",
    "            \n",
    "            # If section is small enough, add as single chunk\n",
    "            if len(section_text) <= self.chunk_size:\n",
    "                chunk = Chunk(\n",
    "                    content=section_text,\n",
    "                    start_index=current_start,\n",
    "                    end_index=current_start + len(section_text),\n",
    "                    chunk_id=f\"chunk_{chunk_id}\",\n",
    "                    metadata={**(metadata or {}), **section_metadata}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                current_start += len(section_text)\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                # Split large section further\n",
    "                sub_chunks = self._split_section(section_text, current_start, \n",
    "                                               {**(metadata or {}), **section_metadata})\n",
    "                chunks.extend(sub_chunks)\n",
    "                current_start += len(section_text)\n",
    "                chunk_id += len(sub_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_by_headers(self, text: str) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"Split text by Markdown headers\"\"\"\n",
    "        sections = []\n",
    "        current_section = \"\"\n",
    "        current_metadata = {}\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # Check if line is a header\n",
    "            if line.startswith('#'):\n",
    "                # Save previous section\n",
    "                if current_section.strip():\n",
    "                    sections.append((current_section.strip(), current_metadata.copy()))\n",
    "                \n",
    "                # Start new section\n",
    "                header_level = len(line) - len(line.lstrip('#'))\n",
    "                header_text = line.lstrip('#').strip()\n",
    "                current_metadata = {\n",
    "                    'header_level': header_level,\n",
    "                    'header_text': header_text,\n",
    "                    'section_type': 'header'\n",
    "                }\n",
    "                current_section = line + '\\n'\n",
    "            else:\n",
    "                current_section += line + '\\n'\n",
    "        \n",
    "        # Add final section\n",
    "        if current_section.strip():\n",
    "            sections.append((current_section.strip(), current_metadata))\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _split_section(self, text: str, start_index: int, metadata: Dict) -> List[Chunk]:\n",
    "        \"\"\"Split a large section into smaller chunks\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_start = start_index\n",
    "        chunk_id = 0\n",
    "        \n",
    "        # Split by paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            if len(current_chunk + paragraph) > self.chunk_size and current_chunk:\n",
    "                # Create chunk\n",
    "                chunk = Chunk(\n",
    "                    content=current_chunk.strip(),\n",
    "                    start_index=current_start,\n",
    "                    end_index=current_start + len(current_chunk),\n",
    "                    chunk_id=f\"chunk_{start_index}_{chunk_id}\",\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_text = current_chunk[-self.chunk_overlap:] if self.chunk_overlap > 0 else \"\"\n",
    "                current_chunk = overlap_text + paragraph\n",
    "                current_start += len(current_chunk) - len(overlap_text)\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk += paragraph + '\\n\\n'\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunk = Chunk(\n",
    "                content=current_chunk.strip(),\n",
    "                start_index=current_start,\n",
    "                end_index=current_start + len(current_chunk),\n",
    "                chunk_id=f\"chunk_{start_index}_{chunk_id}\",\n",
    "                metadata=metadata\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test Markdown splitting\n",
    "markdown_splitter = MarkdownSplitter(chunk_size=300, chunk_overlap=50)\n",
    "markdown_chunks = markdown_splitter.split_text(sample_doc, {\"source\": \"ml_fundamentals\"})\n",
    "\n",
    "print(f\"ðŸ“Š Markdown splitting results:\")\n",
    "print(f\"Number of chunks: {len(markdown_chunks)}\")\n",
    "print(f\"Average chunk length: {np.mean([len(chunk.content) for chunk in markdown_chunks]):.0f} characters\")\n",
    "\n",
    "# Show sample chunks with metadata\n",
    "print(f\"\\nðŸ” Sample Markdown chunks:\")\n",
    "for i, chunk in enumerate(markdown_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (ID: {chunk.chunk_id}):\")\n",
    "    print(f\"Content: {chunk.content[:200]}...\")\n",
    "    print(f\"Length: {len(chunk.content)} characters\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print(f\"Starts with: '{chunk.content[:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbc868",
   "metadata": {},
   "source": [
    "## Code Splitting {#code-splitting}\n",
    "\n",
    "Specialized splitting for code files that preserves syntax and structure.\n",
    "\n",
    "### Pros:\n",
    "- âœ… Preserves code syntax\n",
    "- âœ… Maintains function/class boundaries\n",
    "- âœ… Good for code search and retrieval\n",
    "- âœ… Enables code-specific queries\n",
    "\n",
    "### Cons:\n",
    "- âŒ Language-specific implementation\n",
    "- âŒ Complex parsing requirements\n",
    "- âŒ May create very small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSplitter:\n",
    "    \"\"\"Split code files while preserving syntax structure\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def split_text(self, text: str, metadata: Dict = None) -> List[Chunk]:\n",
    "        \"\"\"Split code text preserving structure\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        # Split by functions/classes first\n",
    "        sections = self._split_by_functions(text)\n",
    "        \n",
    "        for section in sections:\n",
    "            section_text, section_metadata = section\n",
    "            \n",
    "            # If section is small enough, add as single chunk\n",
    "            if len(section_text) <= self.chunk_size:\n",
    "                chunk = Chunk(\n",
    "                    content=section_text,\n",
    "                    start_index=current_start,\n",
    "                    end_index=current_start + len(section_text),\n",
    "                    chunk_id=f\"chunk_{chunk_id}\",\n",
    "                    metadata={**(metadata or {}), **section_metadata}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                current_start += len(section_text)\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                # Split large section further\n",
    "                sub_chunks = self._split_section(section_text, current_start, \n",
    "                                               {**(metadata or {}), **section_metadata})\n",
    "                chunks.extend(sub_chunks)\n",
    "                current_start += len(section_text)\n",
    "                chunk_id += len(sub_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_by_functions(self, text: str) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"Split text by function/class definitions\"\"\"\n",
    "        sections = []\n",
    "        current_section = \"\"\n",
    "        current_metadata = {}\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # Check if line starts a function or class\n",
    "            if (line.strip().startswith('def ') or \n",
    "                line.strip().startswith('class ') or\n",
    "                line.strip().startswith('async def ')):\n",
    "                \n",
    "                # Save previous section\n",
    "                if current_section.strip():\n",
    "                    sections.append((current_section.strip(), current_metadata.copy()))\n",
    "                \n",
    "                # Start new section\n",
    "                if line.strip().startswith('def '):\n",
    "                    func_name = line.strip().split('(')[0].replace('def ', '')\n",
    "                    current_metadata = {\n",
    "                        'type': 'function',\n",
    "                        'name': func_name,\n",
    "                        'section_type': 'function'\n",
    "                    }\n",
    "                elif line.strip().startswith('class '):\n",
    "                    class_name = line.strip().split('(')[0].replace('class ', '').split(':')[0]\n",
    "                    current_metadata = {\n",
    "                        'type': 'class',\n",
    "                        'name': class_name,\n",
    "                        'section_type': 'class'\n",
    "                    }\n",
    "                \n",
    "                current_section = line + '\\n'\n",
    "            else:\n",
    "                current_section += line + '\\n'\n",
    "        \n",
    "        # Add final section\n",
    "        if current_section.strip():\n",
    "            sections.append((current_section.strip(), current_metadata))\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _split_section(self, text: str, start_index: int, metadata: Dict) -> List[Chunk]:\n",
    "        \"\"\"Split a large section into smaller chunks\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_start = start_index\n",
    "        chunk_id = 0\n",
    "        \n",
    "        # Split by logical blocks (indentation changes)\n",
    "        lines = text.split('\\n')\n",
    "        current_block = []\n",
    "        \n",
    "        for line in lines:\n",
    "            current_block.append(line)\n",
    "            \n",
    "            # Check if we should create a chunk\n",
    "            if len('\\n'.join(current_block)) > self.chunk_size and len(current_block) > 1:\n",
    "                # Create chunk from previous block\n",
    "                chunk_text = '\\n'.join(current_block[:-1])\n",
    "                chunk = Chunk(\n",
    "                    content=chunk_text,\n",
    "                    start_index=current_start,\n",
    "                    end_index=current_start + len(chunk_text),\n",
    "                    chunk_id=f\"chunk_{start_index}_{chunk_id}\",\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_lines = current_block[-2:] if len(current_block) >= 2 else current_block\n",
    "                current_block = overlap_lines + [line]\n",
    "                current_start += len(chunk_text)\n",
    "                chunk_id += 1\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_block:\n",
    "            chunk_text = '\\n'.join(current_block)\n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                start_index=current_start,\n",
    "                end_index=current_start + len(chunk_text),\n",
    "                chunk_id=f\"chunk_{start_index}_{chunk_id}\",\n",
    "                metadata=metadata\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Sample Python code\n",
    "sample_code = '''\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document processing and chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def create_product_document(self, product: Dict[str, Any]) -> str:\n",
    "        \"\"\"Convert product data to a searchable document\"\"\"\n",
    "        doc_parts = [\n",
    "            f\"Product: {product['name']}\",\n",
    "            f\"Category: {product['category']}\",\n",
    "            f\"Price: ${product['price']}\",\n",
    "            f\"Description: {product['description']}\"\n",
    "        ]\n",
    "        return \"\\\\n\".join(doc_parts)\n",
    "    \n",
    "    def chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[DocumentChunk]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        \n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            end = min(start + self.chunk_size, len(words))\n",
    "            chunk_words = words[start:end]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "            \n",
    "            chunk_id_str = f\"{metadata.get('product_id', 'unknown')}_chunk_{chunk_id}\"\n",
    "            \n",
    "            chunk = DocumentChunk(\n",
    "                id=chunk_id_str,\n",
    "                content=chunk_text,\n",
    "                metadata=metadata.copy()\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            start = end - self.chunk_overlap\n",
    "            chunk_id += 1\n",
    "            \n",
    "            if start >= len(words) - self.chunk_overlap:\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "def process_documents(documents: List[Dict]) -> List[DocumentChunk]:\n",
    "    \"\"\"Process a list of documents into chunks\"\"\"\n",
    "    processor = DocumentProcessor()\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = processor.chunk_text(doc['content'], doc['metadata'])\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "'''\n",
    "\n",
    "# Test code splitting\n",
    "code_splitter = CodeSplitter(chunk_size=400, chunk_overlap=50)\n",
    "code_chunks = code_splitter.split_text(sample_code, {\"source\": \"document_processor.py\"})\n",
    "\n",
    "print(f\"ðŸ“Š Code splitting results:\")\n",
    "print(f\"Number of chunks: {len(code_chunks)}\")\n",
    "print(f\"Average chunk length: {np.mean([len(chunk.content) for chunk in code_chunks]):.0f} characters\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nðŸ” Sample code chunks:\")\n",
    "for i, chunk in enumerate(code_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (ID: {chunk.chunk_id}):\")\n",
    "    print(f\"Type: {chunk.metadata.get('type', 'unknown')}\")\n",
    "    print(f\"Name: {chunk.metadata.get('name', 'unknown')}\")\n",
    "    print(f\"Content: {chunk.content[:200]}...\")\n",
    "    print(f\"Length: {len(chunk.content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b700a6",
   "metadata": {},
   "source": [
    "## Performance Comparison {#performance}\n",
    "\n",
    "Let's compare all splitting techniques on the same document to understand their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class SplittingEvaluator:\n",
    "    \"\"\"Evaluate different splitting techniques\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.splitters = {\n",
    "            \"Fixed Size (Char)\": FixedSizeSplitter(chunk_size=300, chunk_overlap=50, use_tokens=False),\n",
    "            \"Fixed Size (Token)\": FixedSizeSplitter(chunk_size=150, chunk_overlap=25, use_tokens=True),\n",
    "            \"Semantic\": SemanticSplitter(similarity_threshold=0.6, min_chunk_size=100, max_chunk_size=800),\n",
    "            \"Recursive\": RecursiveSplitter(chunk_size=300, chunk_overlap=50),\n",
    "            \"Sentence Aware\": SentenceAwareSplitter(chunk_size=400, chunk_overlap=100),\n",
    "            \"Markdown\": MarkdownSplitter(chunk_size=300, chunk_overlap=50),\n",
    "            \"Code\": CodeSplitter(chunk_size=400, chunk_overlap=50)\n",
    "        }\n",
    "    \n",
    "    def evaluate_splitter(self, splitter, text: str, metadata: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a single splitter\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            chunks = splitter.split_text(text, metadata)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            chunk_lengths = [len(chunk.content) for chunk in chunks]\n",
    "            chunk_tokens = [len(encoding.encode(chunk.content)) for chunk in chunks]\n",
    "            \n",
    "            # Check for sentence breaks\n",
    "            sentence_breaks = 0\n",
    "            for chunk in chunks:\n",
    "                sentences = sent_tokenize(chunk.content)\n",
    "                if len(sentences) > 1:\n",
    "                    # Check if any sentence is cut off\n",
    "                    for sentence in sentences:\n",
    "                        if not sentence.strip().endswith(('.', '!', '?')):\n",
    "                            sentence_breaks += 1\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"processing_time\": end_time - start_time,\n",
    "                \"num_chunks\": len(chunks),\n",
    "                \"avg_chunk_length\": np.mean(chunk_lengths),\n",
    "                \"std_chunk_length\": np.std(chunk_lengths),\n",
    "                \"min_chunk_length\": np.min(chunk_lengths),\n",
    "                \"max_chunk_length\": np.max(chunk_lengths),\n",
    "                \"avg_chunk_tokens\": np.mean(chunk_tokens),\n",
    "                \"sentence_breaks\": sentence_breaks,\n",
    "                \"chunk_size_consistency\": 1 - (np.std(chunk_lengths) / np.mean(chunk_lengths)) if np.mean(chunk_lengths) > 0 else 0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"processing_time\": 0,\n",
    "                \"num_chunks\": 0,\n",
    "                \"avg_chunk_length\": 0,\n",
    "                \"std_chunk_length\": 0,\n",
    "                \"min_chunk_length\": 0,\n",
    "                \"max_chunk_length\": 0,\n",
    "                \"avg_chunk_tokens\": 0,\n",
    "                \"sentence_breaks\": 0,\n",
    "                \"chunk_size_consistency\": 0\n",
    "            }\n",
    "    \n",
    "    def compare_all(self, text: str, metadata: Dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Compare all splitters\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for name, splitter in self.splitters.items():\n",
    "            print(f\"ðŸ”„ Evaluating {name}...\")\n",
    "            result = self.evaluate_splitter(splitter, text, metadata or {})\n",
    "            result[\"splitter_name\"] = name\n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate all splitters\n",
    "evaluator = SplittingEvaluator()\n",
    "\n",
    "# Test on sample document\n",
    "print(\"ðŸ§ª Evaluating all splitting techniques...\")\n",
    "results_df = evaluator.compare_all(sample_doc, {\"source\": \"ml_fundamentals\"})\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ“Š Performance Comparison Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter successful results\n",
    "successful_results = results_df[results_df[\"success\"] == True].copy()\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    # Sort by processing time\n",
    "    successful_results = successful_results.sort_values(\"processing_time\")\n",
    "    \n",
    "    print(\"\\nâ±ï¸ Processing Time (seconds):\")\n",
    "    for _, row in successful_results.iterrows():\n",
    "        print(f\"  {row['splitter_name']}: {row['processing_time']:.3f}s\")\n",
    "    \n",
    "    print(\"\\nðŸ“ Chunk Statistics:\")\n",
    "    print(f\"{'Splitter':<20} {'Chunks':<8} {'Avg Length':<12} {'Consistency':<12} {'Sentence Breaks':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for _, row in successful_results.iterrows():\n",
    "        print(f\"{row['splitter_name']:<20} {row['num_chunks']:<8} {row['avg_chunk_length']:<12.0f} {row['chunk_size_consistency']:<12.3f} {row['sentence_breaks']:<15}\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Recommendations:\")\n",
    "    print(\"  â€¢ Fastest: Fixed Size splitters\")\n",
    "    print(\"  â€¢ Most consistent: Fixed Size splitters\")\n",
    "    print(\"  â€¢ Best for NLP: Sentence Aware\")\n",
    "    print(\"  â€¢ Best for structure: Markdown/Code splitters\")\n",
    "    print(\"  â€¢ Best for semantics: Semantic splitter\")\n",
    "    \n",
    "    # Show failed splitters\n",
    "    failed_results = results_df[results_df[\"success\"] == False]\n",
    "    if len(failed_results) > 0:\n",
    "        print(\"\\nâŒ Failed Splitters:\")\n",
    "        for _, row in failed_results.iterrows():\n",
    "            print(f\"  {row['splitter_name']}: {row['error']}\")\n",
    "else:\n",
    "    print(\"âŒ All splitters failed!\")\n",
    "    print(\"\\nError details:\")\n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"  {row['splitter_name']}: {row['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec2259",
   "metadata": {},
   "source": [
    "## Best Practices & Guidelines {#best-practices}\n",
    "\n",
    "### Choosing the Right Splitting Technique\n",
    "\n",
    "| Use Case | Recommended Technique | Reason |\n",
    "|----------|----------------------|---------|\n",
    "| **General Text** | Recursive Character | Good balance of structure and size |\n",
    "| **Technical Docs** | Markdown Splitter | Preserves heading hierarchy |\n",
    "| **Code Files** | Code Splitter | Maintains syntax structure |\n",
    "| **Q&A Systems** | Sentence Aware | Preserves sentence integrity |\n",
    "| **Semantic Search** | Semantic Splitter | Groups related content |\n",
    "| **High Performance** | Fixed Size | Fastest processing |\n",
    "\n",
    "### Chunk Size Guidelines\n",
    "\n",
    "| Document Type | Recommended Size | Overlap |\n",
    "|---------------|------------------|---------|\n",
    "| **Short Articles** | 200-400 chars | 50-100 chars |\n",
    "| **Long Documents** | 500-1000 chars | 100-200 chars |\n",
    "| **Code Files** | 300-600 chars | 50-100 chars |\n",
    "| **Technical Docs** | 400-800 chars | 100-150 chars |\n",
    "\n",
    "### Quality Metrics to Monitor\n",
    "\n",
    "1. **Chunk Size Consistency**: Standard deviation / mean\n",
    "2. **Sentence Breaks**: Number of broken sentences\n",
    "3. **Processing Time**: Time to split documents\n",
    "4. **Retrieval Quality**: Precision and recall in RAG\n",
    "5. **Context Preservation**: Semantic coherence within chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471cce1d",
   "metadata": {},
   "source": [
    "## Real-World Examples {#real-world}\n",
    "\n",
    "### 1. E-commerce Product Catalog\n",
    "- **Challenge**: Product descriptions vary greatly in length\n",
    "- **Solution**: Recursive splitting with 300-char chunks\n",
    "- **Result**: Consistent retrieval across different product types\n",
    "\n",
    "### 2. Technical Documentation\n",
    "- **Challenge**: Need to preserve section hierarchy\n",
    "- **Solution**: Markdown splitting with header-aware chunking\n",
    "- **Result**: Better context for technical queries\n",
    "\n",
    "### 3. Legal Documents\n",
    "- **Challenge**: Long paragraphs with complex legal language\n",
    "- **Solution**: Sentence-aware splitting with 400-char chunks\n",
    "- **Result**: Preserved legal context and improved accuracy\n",
    "\n",
    "### 4. Code Repository\n",
    "- **Challenge**: Need to maintain code structure\n",
    "- **Solution**: Code splitting by functions and classes\n",
    "- **Result**: Better code search and retrieval\n",
    "\n",
    "### 5. Customer Support Knowledge Base\n",
    "- **Challenge**: Mix of short FAQs and long articles\n",
    "- **Solution**: Hybrid approach - semantic for articles, fixed-size for FAQs\n",
    "- **Result**: Optimized retrieval for different content types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d578ca6",
   "metadata": {},
   "source": [
    "## Key Takeaways & Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "âœ… **Multiple Splitting Techniques** with different strengths and use cases\n",
    "âœ… **Performance Comparison** showing trade-offs between speed and quality\n",
    "âœ… **Real-world Applications** demonstrating practical implementations\n",
    "âœ… **Best Practices** for choosing the right technique\n",
    "\n",
    "### Key Insights\n",
    "1. **No One-Size-Fits-All**: Choose technique based on your specific use case\n",
    "2. **Chunk Size Matters**: Balance between context and precision\n",
    "3. **Overlap is Important**: Ensures continuity between chunks\n",
    "4. **Structure Preservation**: Consider document type and retrieval needs\n",
    "5. **Performance vs Quality**: Trade-offs between speed and accuracy\n",
    "\n",
    "### Next Steps\n",
    "- **Experiment**: Try different techniques on your specific documents\n",
    "- **Measure**: Implement evaluation metrics for your use case\n",
    "- **Optimize**: Fine-tune parameters based on performance data\n",
    "- **Monitor**: Track retrieval quality and user satisfaction\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "- **Hybrid Splitting**: Combine multiple techniques\n",
    "- **Dynamic Chunking**: Adjust chunk size based on content\n",
    "- **Metadata Enrichment**: Add semantic tags to chunks\n",
    "- **Quality Scoring**: Rate chunk quality automatically\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to implement document splitting?** Start with recursive character splitting for general text, then experiment with specialized techniques based on your specific needs!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
