{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9c6386",
   "metadata": {},
   "source": [
    "# LLM Fundamentals: Understanding Large Language Models\n",
    "\n",
    "## What Are LLMs?\n",
    "Large Language Models (LLMs) are AI systems trained on vast amounts of text data that can understand and generate human-like text. They represent a breakthrough in artificial intelligence, enabling machines to process and produce language in ways that were previously impossible.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand LLM architecture and tokenization\n",
    "- Work with multiple providers (OpenAI, Gemini, Fireworks)\n",
    "- Build real-world applications\n",
    "- Optimize for performance and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('..')\n",
    "from utils.llm_providers import get_available_providers, get_provider_info\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Check available providers\n",
    "providers = get_available_providers()\n",
    "provider_info = get_provider_info()\n",
    "\n",
    "print(f\"‚úÖ Setup complete - {provider_info['available_count']} providers available\")\n",
    "if provider_info['available_providers']:\n",
    "    print(f\"üì° Providers: {', '.join(provider_info['available_providers'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No providers available. Check your API keys in .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49902c41",
   "metadata": {},
   "source": [
    "# 1. Transformer Architecture: How LLMs Think\n",
    "\n",
    "## Key Components\n",
    "- **Embeddings**: Convert text tokens into dense vectors that capture meaning\n",
    "- **Attention**: Focus on relevant parts of the input\n",
    "- **Feed-forward**: Process attended information\n",
    "- **Layer Normalization**: Stabilize training and improve performance\n",
    "\n",
    "## Why This Matters\n",
    "- **Parallel Processing**: Unlike RNNs, transformers process all positions simultaneously\n",
    "- **Long-Range Dependencies**: Attention connects any two positions in the sequence\n",
    "- **Scalability**: Performance improves with more parameters and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b401dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model sizes and capabilities\n",
    "model_info = {\n",
    "    'Small (7B)': {'use_cases': 'Simple tasks, fast inference', 'context': '4K-8K tokens'},\n",
    "    'Medium (13B-70B)': {'use_cases': 'Complex reasoning, code', 'context': '8K-32K tokens'},\n",
    "    'Large (100B+)': {'use_cases': 'Best performance, research', 'context': '32K-1M+ tokens'}\n",
    "}\n",
    "\n",
    "print(\"üîß Model Size Comparison:\")\n",
    "for size, info in model_info.items():\n",
    "    print(f\"   {size}: {info['use_cases']} | Context: {info['context']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1128e8",
   "metadata": {},
   "source": [
    "# 2. Tokenization: How LLMs Understand Text\n",
    "\n",
    "## The Challenge\n",
    "Humans read text as words, but computers need numbers. Tokenization converts human text into tokens (numbers) that LLMs can process.\n",
    "\n",
    "## Popular Methods\n",
    "- **BPE (Byte Pair Encoding)**: Used by GPT models - balances vocabulary size and coverage\n",
    "- **SentencePiece**: Used by many open-source models - handles multiple languages well\n",
    "- **WordPiece**: Used by BERT - good for understanding word boundaries\n",
    "\n",
    "## Impact on Performance\n",
    "- **Token Count**: Affects cost and processing time\n",
    "- **Context Window**: How much text the model can process at once\n",
    "- **Language Support**: Different tokenizers handle different languages better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e409e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different text complexities\n",
    "texts = {\n",
    "    'Simple': \"Hello, how are you?\",\n",
    "    'Medium': \"The quick brown fox jumps over the lazy dog. This is a test sentence.\",\n",
    "    'Complex': \"Large Language Models are AI systems trained on vast text data that can understand and generate human-like text through transformer architecture.\"\n",
    "}\n",
    "\n",
    "def analyze_text_complexity(text):\n",
    "    \"\"\"Simple text analysis\"\"\"\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    estimated_tokens = int(word_count * 1.33)\n",
    "    \n",
    "    complexity = \"Simple\"\n",
    "    if estimated_tokens > 1000:\n",
    "        complexity = \"Complex\"\n",
    "    elif estimated_tokens > 500:\n",
    "        complexity = \"Medium\"\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'estimated_tokens': estimated_tokens,\n",
    "        'complexity': complexity\n",
    "    }\n",
    "\n",
    "print(\"üìä Text Complexity Analysis:\")\n",
    "for name, text in texts.items():\n",
    "    analysis = analyze_text_complexity(text)\n",
    "    print(f\"   {name}: {analysis['word_count']} words, ~{analysis['estimated_tokens']} tokens, {analysis['complexity']} complexity\")\n",
    "\n",
    "print(\"\\nüìè Context Window Impact:\")\n",
    "print(\"   - Small (2K-4K): Basic tasks, short conversations\")\n",
    "print(\"   - Medium (8K-16K): Most applications, documents\")\n",
    "print(\"   - Large (32K-128K): Long documents, codebases\")\n",
    "print(\"   - Very Large (1M+): Entire books, large codebases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22dd7a",
   "metadata": {},
   "source": [
    "# 3. LLM Knowledge: What Do They Know?\n",
    "\n",
    "## Training Data\n",
    "LLMs are trained on massive datasets containing:\n",
    "- **Web pages**: Wikipedia, news articles, blogs\n",
    "- **Books**: Fiction, non-fiction, technical manuals\n",
    "- **Code repositories**: GitHub, Stack Overflow\n",
    "- **Academic papers**: Research and scientific literature\n",
    "\n",
    "## Knowledge Boundaries\n",
    "\n",
    "### **What LLMs Know Well**\n",
    "- **General knowledge**: Facts, concepts, relationships\n",
    "- **Language patterns**: Grammar, style, tone\n",
    "- **Reasoning**: Logical thinking, problem-solving\n",
    "- **Code**: Programming languages and patterns\n",
    "- **Creative tasks**: Writing, storytelling, brainstorming\n",
    "\n",
    "### **What They Struggle With**\n",
    "- **Real-time information**: Events after training cutoff\n",
    "- **Personal information**: Specific details about individuals\n",
    "- **Highly specialized knowledge**: Very niche domains\n",
    "- **Mathematical precision**: Complex calculations\n",
    "- **Consistency**: May contradict themselves\n",
    "\n",
    "## Knowledge Limitations\n",
    "- **Training Cutoff**: Models have a \"knowledge cutoff\" date\n",
    "- **Hallucinations**: Generate plausible but incorrect information\n",
    "- **Bias**: Reflect biases in training data\n",
    "- **Context**: Limited by context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic generation with any available provider\n",
    "if providers:\n",
    "    # Get the first available provider\n",
    "    provider_name = list(providers.keys())[0]\n",
    "    provider = providers[provider_name]\n",
    "    \n",
    "    print(f\"üß™ Testing {provider_name.upper()}...\")\n",
    "    \n",
    "    # Simple test\n",
    "    try:\n",
    "        result = provider.generate(\"Explain AI in one sentence.\", max_tokens=100)\n",
    "        if 'error' not in result:\n",
    "            print(\"‚úÖ LLM Response:\")\n",
    "            print(f\"   {result.get('choices', [{}])[0].get('message', {}).get('content', str(result))}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {result['error']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "else:\n",
    "    print(\"‚ùå No providers available. Check your API keys.\")\n",
    "\n",
    "# Multi-provider comparison if multiple providers available\n",
    "if len(providers) > 1:\n",
    "    print(f\"\\nüîÑ Comparing {len(providers)} providers...\")\n",
    "    test_prompt = \"What is machine learning?\"\n",
    "    \n",
    "    for name, provider in providers.items():\n",
    "        print(f\"\\nü§ñ {name.upper()}:\")\n",
    "        try:\n",
    "            result = provider.generate(test_prompt, max_tokens=150)\n",
    "            if 'error' not in result:\n",
    "                content = result.get('choices', [{}])[0].get('message', {}).get('content', str(result))\n",
    "                print(f\"   {content[:100]}...\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå {result['error']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac59b4",
   "metadata": {},
   "source": [
    "# 4. Real-World Use Cases\n",
    "\n",
    "## Core Capabilities\n",
    "- **Text Generation**: Creative writing, technical documentation, content creation\n",
    "- **Language Understanding**: Translation, summarization, question answering\n",
    "- **Reasoning**: Logical thinking, mathematical problems, code debugging\n",
    "- **Conversation**: Natural interaction, tutoring, customer service\n",
    "\n",
    "## Business Applications\n",
    "- **Content Marketing**: Blog posts, social media, marketing copy\n",
    "- **Customer Support**: Automated responses, FAQ handling\n",
    "- **Data Analysis**: Insights from text data, sentiment analysis\n",
    "- **Code Assistance**: Development productivity, debugging, documentation\n",
    "\n",
    "## Real-World Impact\n",
    "- **60% reduction** in content creation time\n",
    "- **40% faster** development cycles\n",
    "- **80% reduction** in support ticket volume\n",
    "- **Democratizes** data analysis for non-technical users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Generation Assistant\n",
    "def content_assistant(topic, provider):\n",
    "    \"\"\"Generate content for a given topic\"\"\"\n",
    "    try:\n",
    "        # Generate blog ideas\n",
    "        ideas_prompt = f\"Generate 3 blog post ideas about {topic}. Each with title and brief description.\"\n",
    "        ideas = provider.generate(ideas_prompt, max_tokens=200)\n",
    "        \n",
    "        # Create outline\n",
    "        outline_prompt = f\"Create a detailed outline for a blog post about {topic}.\"\n",
    "        outline = provider.generate(outline_prompt, max_tokens=300)\n",
    "        \n",
    "        # Write introduction\n",
    "        intro_prompt = f\"Write an engaging introduction paragraph for a blog post about {topic}.\"\n",
    "        intro = provider.generate(intro_prompt, max_tokens=150)\n",
    "        \n",
    "        return {\n",
    "            'ideas': ideas,\n",
    "            'outline': outline,\n",
    "            'intro': intro\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Test the assistant\n",
    "if providers:\n",
    "    topic = \"AI in Healthcare\"\n",
    "    provider_name = list(providers.keys())[0]\n",
    "    provider = providers[provider_name]\n",
    "    \n",
    "    print(f\"üìù Content Assistant for: {topic}\")\n",
    "    print(f\"ü§ñ Using: {provider_name.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    content = content_assistant(topic, provider)\n",
    "    \n",
    "    if 'error' in content:\n",
    "        print(f\"‚ùå Error: {content['error']}\")\n",
    "    else:\n",
    "        print(\"üí° Blog Ideas:\")\n",
    "        ideas_content = content['ideas'].get('choices', [{}])[0].get('message', {}).get('content', str(content['ideas']))\n",
    "        print(f\"   {ideas_content}\")\n",
    "        \n",
    "        print(\"\\nüìã Outline:\")\n",
    "        outline_content = content['outline'].get('choices', [{}])[0].get('message', {}).get('content', str(content['outline']))\n",
    "        print(f\"   {outline_content}\")\n",
    "        \n",
    "        print(\"\\n‚úçÔ∏è Introduction:\")\n",
    "        intro_content = content['intro'].get('choices', [{}])[0].get('message', {}).get('content', str(content['intro']))\n",
    "        print(f\"   {intro_content}\")\n",
    "else:\n",
    "    print(\"‚ùå No providers available for content generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0559a",
   "metadata": {},
   "source": [
    "# 5. Performance & Cost Analysis\n",
    "\n",
    "## Cost Factors\n",
    "- **Model Size**: Larger models cost more but perform better\n",
    "- **Token Usage**: Each token processed costs money\n",
    "- **Request Volume**: High-volume applications need cost optimization\n",
    "- **Provider Choice**: Different providers have different pricing\n",
    "\n",
    "## Optimization Strategies\n",
    "- **Model Selection**: Choose the right model for the task\n",
    "- **Prompt Engineering**: Write efficient prompts to reduce tokens\n",
    "- **Caching**: Cache responses for repeated queries\n",
    "- **Batching**: Process multiple requests together\n",
    "\n",
    "## Production Considerations\n",
    "- **Error Handling**: Retry logic, fallback providers\n",
    "- **Rate Limiting**: Respect API limits\n",
    "- **Monitoring**: Track usage, errors, and performance\n",
    "- **Security**: API key management, input validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23758ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost analysis for different usage scenarios\n",
    "def estimate_costs():\n",
    "    \"\"\"Estimate costs for different usage scenarios\"\"\"\n",
    "    scenarios = {\n",
    "        'Light': {'requests/day': 100, 'tokens/request': 200},\n",
    "        'Medium': {'requests/day': 1000, 'tokens/request': 500},\n",
    "        'Heavy': {'requests/day': 10000, 'tokens/request': 1000}\n",
    "    }\n",
    "    \n",
    "    # Approximate costs per 1K tokens\n",
    "    costs = {\n",
    "        'gpt-4o-mini': 0.00015,\n",
    "        'gpt-4o': 0.005,\n",
    "        'gemini-1.5-flash': 0.000075,\n",
    "        'gemini-1.5-pro': 0.00125\n",
    "    }\n",
    "    \n",
    "    print(\"üí∞ Cost Analysis:\")\n",
    "    for scenario, data in scenarios.items():\n",
    "        daily_tokens = data['requests/day'] * data['tokens/request']\n",
    "        print(f\"\\nüìä {scenario} Usage ({data['requests/day']:,} requests/day):\")\n",
    "        \n",
    "        for model, cost_per_1k in costs.items():\n",
    "            daily_cost = (daily_tokens / 1000) * cost_per_1k\n",
    "            monthly_cost = daily_cost * 30\n",
    "            print(f\"   {model}: ${daily_cost:.2f}/day, ${monthly_cost:.2f}/month\")\n",
    "\n",
    "estimate_costs()\n",
    "\n",
    "# Usage Statistics\n",
    "if providers:\n",
    "    print(f\"\\nüìà Usage Statistics:\")\n",
    "    for provider_name, provider in providers.items():\n",
    "        stats = provider.get_stats()\n",
    "        print(f\"   {provider_name}: {stats['successful_requests']} requests, {stats['total_tokens']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3e89f",
   "metadata": {},
   "source": [
    "# 6. Summary & Next Steps\n",
    "\n",
    "## Key Takeaways\n",
    "- **LLMs solve**: Context understanding, language generation, task generalization\n",
    "- **Architecture**: Transformer-based with attention mechanisms\n",
    "- **Providers**: Work with any available (OpenAI, Gemini, Fireworks)\n",
    "- **Production**: Error handling, monitoring, cost optimization\n",
    "\n",
    "## What You've Built\n",
    "- Multi-provider LLM system\n",
    "- Content generation assistant\n",
    "- Cost analysis tools\n",
    "- Production-ready patterns\n",
    "\n",
    "## Next Steps\n",
    "- **Week 1**: Prompt Engineering, RAG Systems, Streaming Apps\n",
    "- **Week 2**: Conversational AI, Voice Systems, Memory Management\n",
    "- **Week 3**: AI Agents, Multi-Agent Systems, Workflows\n",
    "\n",
    "## Quick Reference\n",
    "```python\n",
    "# Basic usage\n",
    "result = provider.generate(\"Your prompt here\")\n",
    "\n",
    "# With fallback\n",
    "result = robust_generation(\"Your prompt here\", providers)\n",
    "\n",
    "# Get stats\n",
    "stats = provider.get_stats()\n",
    "```\n",
    "\n",
    "**Ready for the next challenge?** Move to `02_prompt_engineering.ipynb`! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
