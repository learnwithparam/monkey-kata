{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4805ea3",
   "metadata": {},
   "source": [
    "# RAG Optimization: Performance, Cost, and Quality Tuning\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to RAG Optimization](#introduction)\n",
    "2. [Performance Optimization](#performance-optimization)\n",
    "3. [Cost Optimization](#cost-optimization)\n",
    "4. [Quality Optimization](#quality-optimization)\n",
    "5. [Caching Strategies](#caching-strategies)\n",
    "6. [Batch Processing](#batch-processing)\n",
    "7. [Model Selection](#model-selection)\n",
    "8. [Infrastructure Optimization](#infrastructure-optimization)\n",
    "9. [Monitoring & Metrics](#monitoring)\n",
    "10. [Real-World Optimization Cases](#real-world-cases)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to RAG Optimization {#introduction}\n",
    "\n",
    "RAG optimization focuses on improving three key aspects of RAG systems:\n",
    "\n",
    "### 1. Performance Optimization\n",
    "- **Latency Reduction**: Faster response times\n",
    "- **Throughput Improvement**: Handle more requests\n",
    "- **Resource Efficiency**: Better CPU/memory usage\n",
    "- **Scalability**: Handle growing workloads\n",
    "\n",
    "### 2. Cost Optimization\n",
    "- **API Cost Reduction**: Minimize LLM API calls\n",
    "- **Infrastructure Costs**: Optimize compute resources\n",
    "- **Storage Costs**: Efficient data storage\n",
    "- **Operational Costs**: Reduce maintenance overhead\n",
    "\n",
    "### 3. Quality Optimization\n",
    "- **Accuracy Improvement**: Better retrieval and generation\n",
    "- **Relevance Enhancement**: More relevant responses\n",
    "- **Consistency**: Reliable performance across queries\n",
    "- **User Satisfaction**: Better user experience\n",
    "\n",
    "### Optimization Trade-offs\n",
    "- **Performance vs Quality**: Faster responses may reduce quality\n",
    "- **Cost vs Performance**: Better performance often costs more\n",
    "- **Quality vs Cost**: Higher quality may require more resources\n",
    "- **Latency vs Throughput**: Lower latency may reduce throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeeedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sentence-transformers qdrant-client openai python-dotenv tiktoken asyncio redis psutil memory-profiler\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import psutil\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import redis\n",
    "import pickle\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")\n",
    "print(\"ðŸ”§ Environment configured for RAG optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148251d4",
   "metadata": {},
   "source": [
    "## Performance Optimization {#performance-optimization}\n",
    "\n",
    "Performance optimization focuses on reducing latency and improving throughput while maintaining quality.\n",
    "\n",
    "### Key Optimization Areas\n",
    "\n",
    "1. **Embedding Optimization**: Faster embedding generation\n",
    "2. **Vector Search Optimization**: Efficient similarity search\n",
    "3. **LLM Optimization**: Faster text generation\n",
    "4. **Caching**: Reduce redundant computations\n",
    "5. **Parallel Processing**: Concurrent operations\n",
    "6. **Memory Management**: Efficient memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36889afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Performance metrics for RAG system\"\"\"\n",
    "    total_time: float\n",
    "    embedding_time: float\n",
    "    retrieval_time: float\n",
    "    generation_time: float\n",
    "    memory_usage: float\n",
    "    cpu_usage: float\n",
    "    tokens_generated: int\n",
    "    tokens_processed: int\n",
    "\n",
    "class OptimizedRAGSystem:\n",
    "    \"\"\"Optimized RAG system with performance monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 use_gpu: bool = False,\n",
    "                 batch_size: int = 32):\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        if use_gpu:\n",
    "            self.embedder = self.embedder.cuda()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vector_client = QdrantClient(\":memory:\")\n",
    "        self.vector_client.create_collection(\n",
    "            collection_name=\"optimized_kb\",\n",
    "            vectors_config=VectorParams(\n",
    "                size=self.embedder.get_sentence_embedding_dimension(),\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.performance_history = []\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "        print(f\"âœ… Optimized RAG system initialized with {embedding_model}\")\n",
    "        print(f\"ðŸ”§ GPU acceleration: {use_gpu}\")\n",
    "        print(f\"ðŸ“¦ Batch size: {batch_size}\")\n",
    "    \n",
    "    def add_documents_batch(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"Add documents in optimized batches\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process documents in batches\n",
    "        for i in range(0, len(documents), self.batch_size):\n",
    "            batch = documents[i:i + self.batch_size]\n",
    "            \n",
    "            # Generate embeddings in batch\n",
    "            batch_texts = [doc[\"content\"] for doc in batch]\n",
    "            batch_embeddings = self.embedder.encode(batch_texts, batch_size=self.batch_size)\n",
    "            \n",
    "            # Create points\n",
    "            points = []\n",
    "            for j, (doc, embedding) in enumerate(zip(batch, batch_embeddings)):\n",
    "                point = PointStruct(\n",
    "                    id=i + j,\n",
    "                    vector=embedding.tolist(),\n",
    "                    payload={\n",
    "                        \"content\": doc[\"content\"],\n",
    "                        \"metadata\": doc.get(\"metadata\", {}),\n",
    "                        \"doc_id\": doc.get(\"id\", f\"doc_{i + j}\")\n",
    "                    }\n",
    "                )\n",
    "                points.append(point)\n",
    "            \n",
    "            # Add to vector store\n",
    "            self.vector_client.upsert(\n",
    "                collection_name=\"optimized_kb\",\n",
    "                points=points\n",
    "            )\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"âœ… Added {len(documents)} documents in {processing_time:.2f}s\")\n",
    "        print(f\"ðŸ“Š Average time per document: {processing_time / len(documents):.4f}s\")\n",
    "    \n",
    "    async def search_optimized(self, query: str, limit: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Optimized search with performance monitoring\"\"\"\n",
    "        start_time = time.time()\n",
    "        memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        cpu_before = psutil.cpu_percent()\n",
    "        \n",
    "        # Generate query embedding\n",
    "        embedding_start = time.time()\n",
    "        query_embedding = self.embedder.encode([query], batch_size=1)[0]\n",
    "        embedding_time = time.time() - embedding_start\n",
    "        \n",
    "        # Vector search\n",
    "        retrieval_start = time.time()\n",
    "        search_results = self.vector_client.search(\n",
    "            collection_name=\"optimized_kb\",\n",
    "            query_vector=query_embedding.tolist(),\n",
    "            limit=limit\n",
    "        )\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        \n",
    "        # Generate response\n",
    "        generation_start = time.time()\n",
    "        context = \"\\n\".join([hit.payload[\"content\"] for hit in search_results])\n",
    "        \n",
    "        # Simple response generation (in practice, use LLM)\n",
    "        response = f\"Based on the retrieved information: {context[:200]}...\"\n",
    "        generation_time = time.time() - generation_start\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_time = time.time() - start_time\n",
    "        memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        cpu_after = psutil.cpu_percent()\n",
    "        \n",
    "        metrics = PerformanceMetrics(\n",
    "            total_time=total_time,\n",
    "            embedding_time=embedding_time,\n",
    "            retrieval_time=retrieval_time,\n",
    "            generation_time=generation_time,\n",
    "            memory_usage=memory_after - memory_before,\n",
    "            cpu_usage=(cpu_before + cpu_after) / 2,\n",
    "            tokens_generated=len(response.split()),\n",
    "            tokens_processed=len(query.split()) + len(context.split())\n",
    "        )\n",
    "        \n",
    "        self.performance_history.append(metrics)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"results\": [hit.payload for hit in search_results],\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance summary\"\"\"\n",
    "        if not self.performance_history:\n",
    "            return {\"message\": \"No performance data available\"}\n",
    "        \n",
    "        metrics = self.performance_history[-10:]  # Last 10 queries\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.performance_history),\n",
    "            \"average_total_time\": np.mean([m.total_time for m in metrics]),\n",
    "            \"average_embedding_time\": np.mean([m.embedding_time for m in metrics]),\n",
    "            \"average_retrieval_time\": np.mean([m.retrieval_time for m in metrics]),\n",
    "            \"average_generation_time\": np.mean([m.generation_time for m in metrics]),\n",
    "            \"average_memory_usage\": np.mean([m.memory_usage for m in metrics]),\n",
    "            \"average_cpu_usage\": np.mean([m.cpu_usage for m in metrics]),\n",
    "            \"average_tokens_generated\": np.mean([m.tokens_generated for m in metrics]),\n",
    "            \"average_tokens_processed\": np.mean([m.tokens_processed for m in metrics]),\n",
    "            \"cache_hit_rate\": self.cache_hits / max(self.cache_hits + self.cache_misses, 1)\n",
    "        }\n",
    "    \n",
    "    def optimize_embeddings(self, queries: List[str]) -> List[np.ndarray]:\n",
    "        \"\"\"Optimized batch embedding generation\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process in batches\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(queries), self.batch_size):\n",
    "            batch = queries[i:i + self.batch_size]\n",
    "            batch_embeddings = self.embedder.encode(batch, batch_size=self.batch_size)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"âœ… Generated {len(queries)} embeddings in {processing_time:.2f}s\")\n",
    "        print(f\"ðŸ“Š Average time per embedding: {processing_time / len(queries):.4f}s\")\n",
    "        \n",
    "        return all_embeddings\n",
    "\n",
    "# Test optimized RAG system\n",
    "print(\"ðŸ§ª Testing Optimized RAG System:\")\n",
    "\n",
    "# Create optimized system\n",
    "optimized_rag = OptimizedRAGSystem(use_gpu=False, batch_size=16)\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    {\n",
    "        \"id\": f\"doc_{i}\",\n",
    "        \"content\": f\"Document {i}: This is sample content about machine learning, artificial intelligence, and data science. It contains information about algorithms, models, and applications.\",\n",
    "        \"metadata\": {\"category\": \"AI\", \"type\": \"technical\"}\n",
    "    }\n",
    "    for i in range(100)  # 100 documents for testing\n",
    "]\n",
    "\n",
    "# Add documents with batch processing\n",
    "optimized_rag.add_documents_batch(sample_docs)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does artificial intelligence work?\",\n",
    "    \"What are data science applications?\",\n",
    "    \"Explain machine learning algorithms\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "# Test performance\n",
    "print(f\"\\nðŸ” Testing performance with {len(test_queries)} queries:\")\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\nQuery {i+1}: '{query}'\")\n",
    "    \n",
    "    result = await optimized_rag.search_optimized(query, limit=3)\n",
    "    \n",
    "    metrics = result[\"metrics\"]\n",
    "    print(f\"  Total time: {metrics.total_time:.3f}s\")\n",
    "    print(f\"  Embedding time: {metrics.embedding_time:.3f}s\")\n",
    "    print(f\"  Retrieval time: {metrics.retrieval_time:.3f}s\")\n",
    "    print(f\"  Generation time: {metrics.generation_time:.3f}s\")\n",
    "    print(f\"  Memory usage: {metrics.memory_usage:.2f}MB\")\n",
    "    print(f\"  CPU usage: {metrics.cpu_usage:.1f}%\")\n",
    "\n",
    "# Get performance summary\n",
    "print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "summary = optimized_rag.get_performance_summary()\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test batch embedding optimization\n",
    "print(f\"\\nðŸ”„ Testing batch embedding optimization:\")\n",
    "batch_queries = [f\"Query {i} about machine learning\" for i in range(50)]\n",
    "embeddings = optimized_rag.optimize_embeddings(batch_queries)\n",
    "print(f\"Generated {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e62527",
   "metadata": {},
   "source": [
    "## Cost Optimization {#cost-optimization}\n",
    "\n",
    "Cost optimization focuses on reducing operational costs while maintaining system performance and quality.\n",
    "\n",
    "### Key Cost Factors\n",
    "\n",
    "1. **LLM API Costs**: Token usage and API calls\n",
    "2. **Embedding Costs**: Vector generation and storage\n",
    "3. **Infrastructure Costs**: Compute, storage, and networking\n",
    "4. **Operational Costs**: Maintenance and monitoring\n",
    "5. **Data Costs**: Storage and transfer costs\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "1. **Token Optimization**: Reduce token usage\n",
    "2. **Caching**: Avoid redundant computations\n",
    "3. **Model Selection**: Choose cost-effective models\n",
    "4. **Batch Processing**: Reduce API call overhead\n",
    "5. **Compression**: Reduce data storage and transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostOptimizedRAG:\n",
    "    \"\"\"Cost-optimized RAG system with cost tracking and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 llm_model: str = \"gpt-3.5-turbo\",\n",
    "                 use_caching: bool = True):\n",
    "        \n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        self.llm_model = llm_model\n",
    "        self.use_caching = use_caching\n",
    "        \n",
    "        # Cost tracking\n",
    "        self.cost_metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_cost\": 0.0,\n",
    "            \"embedding_cost\": 0.0,\n",
    "            \"llm_cost\": 0.0,\n",
    "            \"cache_savings\": 0.0\n",
    "        }\n",
    "        \n",
    "        # Token costs (approximate)\n",
    "        self.token_costs = {\n",
    "            \"gpt-3.5-turbo\": 0.002,  # $0.002 per 1K tokens\n",
    "            \"gpt-4\": 0.03,           # $0.03 per 1K tokens\n",
    "            \"embedding\": 0.0001      # $0.0001 per 1K tokens\n",
    "        }\n",
    "        \n",
    "        # Caching\n",
    "        if use_caching:\n",
    "            self.query_cache = {}\n",
    "            self.embedding_cache = {}\n",
    "            self.response_cache = {}\n",
    "        \n",
    "        print(f\"âœ… Cost-optimized RAG initialized\")\n",
    "        print(f\"ðŸ’° LLM model: {llm_model}\")\n",
    "        print(f\"ðŸ’¾ Caching enabled: {use_caching}\")\n",
    "    \n",
    "    def calculate_token_cost(self, text: str, model: str) -> float:\n",
    "        \"\"\"Calculate cost for text based on model\"\"\"\n",
    "        # Simple token counting (in practice, use tiktoken)\n",
    "        token_count = len(text.split()) * 1.3  # Approximate token count\n",
    "        \n",
    "        if model in self.token_costs:\n",
    "            cost_per_token = self.token_costs[model] / 1000\n",
    "            return token_count * cost_per_token\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def calculate_embedding_cost(self, text: str) -> float:\n",
    "        \"\"\"Calculate cost for embedding generation\"\"\"\n",
    "        return self.calculate_token_cost(text, \"embedding\")\n",
    "    \n",
    "    def calculate_llm_cost(self, text: str) -> float:\n",
    "        \"\"\"Calculate cost for LLM generation\"\"\"\n",
    "        return self.calculate_token_cost(text, self.llm_model)\n",
    "    \n",
    "    def get_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate cache key for query\"\"\"\n",
    "        return hashlib.md5(query.encode()).hexdigest()\n",
    "    \n",
    "    def search_with_cost_optimization(self, query: str, limit: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Search with cost optimization\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.cost_metrics[\"total_queries\"] += 1\n",
    "        \n",
    "        # Check cache first\n",
    "        if self.use_caching:\n",
    "            cache_key = self.get_cache_key(query)\n",
    "            if cache_key in self.query_cache:\n",
    "                self.cost_metrics[\"cache_savings\"] += 1\n",
    "                print(f\"ðŸ’¾ Cache hit for query: {query[:50]}...\")\n",
    "                return self.query_cache[cache_key]\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_cost = self.calculate_embedding_cost(query)\n",
    "        self.cost_metrics[\"embedding_cost\"] += query_cost\n",
    "        \n",
    "        # Mock retrieval (in practice, use vector store)\n",
    "        retrieval_results = [\n",
    "            {\"content\": f\"Result {i} for query: {query}\", \"score\": 0.9 - i * 0.1}\n",
    "            for i in range(limit)\n",
    "        ]\n",
    "        \n",
    "        # Generate response\n",
    "        context = \"\\n\".join([r[\"content\"] for r in retrieval_results])\n",
    "        response = f\"Based on the retrieved information: {context[:200]}...\"\n",
    "        \n",
    "        response_cost = self.calculate_llm_cost(response)\n",
    "        self.cost_metrics[\"llm_cost\"] += response_cost\n",
    "        \n",
    "        # Calculate total cost\n",
    "        total_cost = query_cost + response_cost\n",
    "        self.cost_metrics[\"total_cost\"] += total_cost\n",
    "        self.cost_metrics[\"total_tokens\"] += len(query.split()) + len(response.split())\n",
    "        \n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"results\": retrieval_results,\n",
    "            \"cost_breakdown\": {\n",
    "                \"query_cost\": query_cost,\n",
    "                \"response_cost\": response_cost,\n",
    "                \"total_cost\": total_cost\n",
    "            },\n",
    "            \"processing_time\": time.time() - start_time\n",
    "        }\n",
    "        \n",
    "        # Cache result\n",
    "        if self.use_caching:\n",
    "            self.query_cache[cache_key] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cost_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cost summary\"\"\"\n",
    "        if self.cost_metrics[\"total_queries\"] == 0:\n",
    "            return {\"message\": \"No cost data available\"}\n",
    "        \n",
    "        avg_cost_per_query = self.cost_metrics[\"total_cost\"] / self.cost_metrics[\"total_queries\"]\n",
    "        avg_tokens_per_query = self.cost_metrics[\"total_tokens\"] / self.cost_metrics[\"total_queries\"]\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": self.cost_metrics[\"total_queries\"],\n",
    "            \"total_cost\": self.cost_metrics[\"total_cost\"],\n",
    "            \"average_cost_per_query\": avg_cost_per_query,\n",
    "            \"total_tokens\": self.cost_metrics[\"total_tokens\"],\n",
    "            \"average_tokens_per_query\": avg_tokens_per_query,\n",
    "            \"embedding_cost\": self.cost_metrics[\"embedding_cost\"],\n",
    "            \"llm_cost\": self.cost_metrics[\"llm_cost\"],\n",
    "            \"cache_hit_rate\": self.cost_metrics[\"cache_savings\"] / self.cost_metrics[\"total_queries\"],\n",
    "            \"cost_breakdown\": {\n",
    "                \"embedding_percentage\": (self.cost_metrics[\"embedding_cost\"] / self.cost_metrics[\"total_cost\"]) * 100,\n",
    "                \"llm_percentage\": (self.cost_metrics[\"llm_cost\"] / self.cost_metrics[\"total_cost\"]) * 100\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def optimize_for_cost(self, queries: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize system for cost reduction\"\"\"\n",
    "        optimizations = {\n",
    "            \"batch_processing\": False,\n",
    "            \"query_compression\": False,\n",
    "            \"response_compression\": False,\n",
    "            \"model_downgrade\": False,\n",
    "            \"caching_improvement\": False\n",
    "        }\n",
    "        \n",
    "        # Analyze queries for optimization opportunities\n",
    "        if len(queries) > 10:\n",
    "            optimizations[\"batch_processing\"] = True\n",
    "        \n",
    "        # Check for similar queries\n",
    "        if self.use_caching:\n",
    "            cache_hit_rate = self.cost_metrics[\"cache_savings\"] / max(self.cost_metrics[\"total_queries\"], 1)\n",
    "            if cache_hit_rate < 0.3:\n",
    "                optimizations[\"caching_improvement\"] = True\n",
    "        \n",
    "        # Check for long queries\n",
    "        avg_query_length = np.mean([len(q.split()) for q in queries])\n",
    "        if avg_query_length > 20:\n",
    "            optimizations[\"query_compression\"] = True\n",
    "        \n",
    "        # Check for long responses\n",
    "        if self.cost_metrics[\"total_tokens\"] > 0:\n",
    "            avg_response_length = self.cost_metrics[\"total_tokens\"] / self.cost_metrics[\"total_queries\"]\n",
    "            if avg_response_length > 100:\n",
    "                optimizations[\"response_compression\"] = True\n",
    "        \n",
    "        # Check if model upgrade is needed\n",
    "        if self.llm_model == \"gpt-3.5-turbo\" and self.cost_metrics[\"total_cost\"] > 10:\n",
    "            optimizations[\"model_downgrade\"] = True\n",
    "        \n",
    "        return optimizations\n",
    "\n",
    "# Test cost-optimized RAG\n",
    "print(\"ðŸ§ª Testing Cost-Optimized RAG System:\")\n",
    "\n",
    "# Create cost-optimized system\n",
    "cost_rag = CostOptimizedRAG(use_caching=True)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does artificial intelligence work?\",\n",
    "    \"What is machine learning?\",  # Duplicate to test caching\n",
    "    \"Explain deep learning algorithms\",\n",
    "    \"What is machine learning?\",  # Another duplicate\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is data science?\",\n",
    "    \"Explain machine learning models\",\n",
    "    \"What is machine learning?\",  # Another duplicate\n",
    "    \"How does reinforcement learning work?\"\n",
    "]\n",
    "\n",
    "print(f\"ðŸ” Testing cost optimization with {len(test_queries)} queries:\")\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\nQuery {i+1}: '{query}'\")\n",
    "    \n",
    "    result = cost_rag.search_with_cost_optimization(query, limit=3)\n",
    "    \n",
    "    cost_breakdown = result[\"cost_breakdown\"]\n",
    "    print(f\"  Query cost: ${cost_breakdown['query_cost']:.6f}\")\n",
    "    print(f\"  Response cost: ${cost_breakdown['response_cost']:.6f}\")\n",
    "    print(f\"  Total cost: ${cost_breakdown['total_cost']:.6f}\")\n",
    "    print(f\"  Processing time: {result['processing_time']:.3f}s\")\n",
    "\n",
    "# Get cost summary\n",
    "print(f\"\\nðŸ’° Cost Summary:\")\n",
    "summary = cost_rag.get_cost_summary()\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Get optimization recommendations\n",
    "print(f\"\\nðŸ”§ Optimization Recommendations:\")\n",
    "optimizations = cost_rag.optimize_for_cost(test_queries)\n",
    "for optimization, recommended in optimizations.items():\n",
    "    status = \"âœ… Recommended\" if recommended else \"âŒ Not needed\"\n",
    "    print(f\"  {optimization}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38231ca",
   "metadata": {},
   "source": [
    "## Quality Optimization {#quality-optimization}\n",
    "\n",
    "Quality optimization focuses on improving the accuracy, relevance, and coherence of RAG responses.\n",
    "\n",
    "### Key Quality Metrics\n",
    "\n",
    "1. **Accuracy**: Factual correctness of responses\n",
    "2. **Relevance**: How well responses address queries\n",
    "3. **Completeness**: Coverage of query topics\n",
    "4. **Coherence**: Logical flow and structure\n",
    "5. **Consistency**: Reliable performance across queries\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "1. **Retrieval Enhancement**: Improve document retrieval\n",
    "2. **Context Optimization**: Better context selection\n",
    "3. **Response Refinement**: Improve response generation\n",
    "4. **Quality Monitoring**: Track and improve quality\n",
    "5. **Feedback Integration**: Learn from user feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityOptimizedRAG:\n",
    "    \"\"\"Quality-optimized RAG system with quality monitoring and improvement\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 quality_threshold: float = 0.7):\n",
    "        \n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        self.quality_threshold = quality_threshold\n",
    "        \n",
    "        # Quality tracking\n",
    "        self.quality_metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"high_quality_responses\": 0,\n",
    "            \"medium_quality_responses\": 0,\n",
    "            \"low_quality_responses\": 0,\n",
    "            \"average_quality_score\": 0.0,\n",
    "            \"quality_improvements\": 0\n",
    "        }\n",
    "        \n",
    "        # Quality assessment criteria\n",
    "        self.quality_criteria = {\n",
    "            \"relevance\": 0.3,\n",
    "            \"completeness\": 0.25,\n",
    "            \"accuracy\": 0.25,\n",
    "            \"coherence\": 0.2\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Quality-optimized RAG initialized\")\n",
    "        print(f\"ðŸŽ¯ Quality threshold: {quality_threshold}\")\n",
    "    \n",
    "    def assess_response_quality(self, query: str, response: str, context: str) -> Dict[str, float]:\n",
    "        \"\"\"Assess quality of response across multiple criteria\"\"\"\n",
    "        \n",
    "        # Relevance assessment\n",
    "        relevance_score = self._assess_relevance(query, response)\n",
    "        \n",
    "        # Completeness assessment\n",
    "        completeness_score = self._assess_completeness(query, response)\n",
    "        \n",
    "        # Accuracy assessment\n",
    "        accuracy_score = self._assess_accuracy(response, context)\n",
    "        \n",
    "        # Coherence assessment\n",
    "        coherence_score = self._assess_coherence(response)\n",
    "        \n",
    "        # Calculate overall quality score\n",
    "        overall_score = (\n",
    "            relevance_score * self.quality_criteria[\"relevance\"] +\n",
    "            completeness_score * self.quality_criteria[\"completeness\"] +\n",
    "            accuracy_score * self.quality_criteria[\"accuracy\"] +\n",
    "            coherence_score * self.quality_criteria[\"coherence\"]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"overall_score\": overall_score,\n",
    "            \"relevance\": relevance_score,\n",
    "            \"completeness\": completeness_score,\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"coherence\": coherence_score\n",
    "        }\n",
    "    \n",
    "    def _assess_relevance(self, query: str, response: str) -> float:\n",
    "        \"\"\"Assess relevance of response to query\"\"\"\n",
    "        query_terms = set(query.lower().split())\n",
    "        response_terms = set(response.lower().split())\n",
    "        \n",
    "        if not query_terms:\n",
    "            return 0.5\n",
    "        \n",
    "        # Calculate term overlap\n",
    "        overlap = len(query_terms & response_terms)\n",
    "        relevance_score = overlap / len(query_terms)\n",
    "        \n",
    "        return min(relevance_score, 1.0)\n",
    "    \n",
    "    def _assess_completeness(self, query: str, response: str) -> float:\n",
    "        \"\"\"Assess completeness of response\"\"\"\n",
    "        query_terms = set(query.lower().split())\n",
    "        response_terms = set(response.lower().split())\n",
    "        \n",
    "        if not query_terms:\n",
    "            return 0.5\n",
    "        \n",
    "        # Check if response addresses query terms\n",
    "        addressed_terms = len(query_terms & response_terms)\n",
    "        completeness_score = addressed_terms / len(query_terms)\n",
    "        \n",
    "        # Also consider response length\n",
    "        length_score = min(len(response) / 200, 1.0)  # Normalize to 200 chars\n",
    "        \n",
    "        return (completeness_score + length_score) / 2\n",
    "    \n",
    "    def _assess_accuracy(self, response: str, context: str) -> float:\n",
    "        \"\"\"Assess accuracy of response against context\"\"\"\n",
    "        if not context:\n",
    "            return 0.5\n",
    "        \n",
    "        # Simple accuracy check: count fact claims vs context\n",
    "        response_sentences = response.split('. ')\n",
    "        context_sentences = context.split('. ')\n",
    "        \n",
    "        if not response_sentences or not context_sentences:\n",
    "            return 0.5\n",
    "        \n",
    "        # Check if response sentences are supported by context\n",
    "        supported_sentences = 0\n",
    "        for resp_sentence in response_sentences:\n",
    "            if any(resp_sentence.lower() in ctx_sentence.lower() for ctx_sentence in context_sentences):\n",
    "                supported_sentences += 1\n",
    "        \n",
    "        accuracy_score = supported_sentences / len(response_sentences)\n",
    "        return accuracy_score\n",
    "    \n",
    "    def _assess_coherence(self, response: str) -> float:\n",
    "        \"\"\"Assess coherence of response\"\"\"\n",
    "        sentences = response.split('. ')\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return 0.5\n",
    "        \n",
    "        # Check for transition words\n",
    "        transition_words = [\"however\", \"therefore\", \"furthermore\", \"additionally\", \"moreover\", \"consequently\"]\n",
    "        transition_count = sum(1 for word in transition_words if word in response.lower())\n",
    "        \n",
    "        # Check sentence length variation\n",
    "        sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "        length_variation = np.std(sentence_lengths) / np.mean(sentence_lengths) if np.mean(sentence_lengths) > 0 else 0\n",
    "        \n",
    "        # Coherence score\n",
    "        transition_score = min(transition_count / len(sentences), 1.0)\n",
    "        variation_score = min(length_variation, 1.0)\n",
    "        \n",
    "        return (transition_score + variation_score) / 2\n",
    "    \n",
    "    def improve_response_quality(self, query: str, response: str, context: str) -> str:\n",
    "        \"\"\"Improve response quality based on assessment\"\"\"\n",
    "        quality_scores = self.assess_response_quality(query, response, context)\n",
    "        \n",
    "        improved_response = response\n",
    "        \n",
    "        # Improve relevance\n",
    "        if quality_scores[\"relevance\"] < 0.6:\n",
    "            improved_response = self._improve_relevance(query, improved_response)\n",
    "        \n",
    "        # Improve completeness\n",
    "        if quality_scores[\"completeness\"] < 0.6:\n",
    "            improved_response = self._improve_completeness(query, improved_response)\n",
    "        \n",
    "        # Improve coherence\n",
    "        if quality_scores[\"coherence\"] < 0.6:\n",
    "            improved_response = self._improve_coherence(improved_response)\n",
    "        \n",
    "        return improved_response\n",
    "    \n",
    "    def _improve_relevance(self, query: str, response: str) -> str:\n",
    "        \"\"\"Improve relevance of response to query\"\"\"\n",
    "        query_terms = set(query.lower().split())\n",
    "        response_terms = set(response.lower().split())\n",
    "        \n",
    "        # Add missing query terms to response\n",
    "        missing_terms = query_terms - response_terms\n",
    "        if missing_terms:\n",
    "            improved_response = f\"{response} This addresses: {', '.join(missing_terms)}.\"\n",
    "            return improved_response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _improve_completeness(self, query: str, response: str) -> str:\n",
    "        \"\"\"Improve completeness of response\"\"\"\n",
    "        if len(response) < 100:\n",
    "            improved_response = f\"{response} For more details, consider exploring related topics and applications.\"\n",
    "            return improved_response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _improve_coherence(self, response: str) -> str:\n",
    "        \"\"\"Improve coherence of response\"\"\"\n",
    "        sentences = response.split('. ')\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return response\n",
    "        \n",
    "        # Add transition words\n",
    "        improved_sentences = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i > 0 and not any(word in sentence.lower() for word in [\"however\", \"therefore\", \"furthermore\", \"additionally\"]):\n",
    "                improved_sentences.append(f\"Furthermore, {sentence}\")\n",
    "            else:\n",
    "                improved_sentences.append(sentence)\n",
    "        \n",
    "        return '. '.join(improved_sentences)\n",
    "    \n",
    "    def search_with_quality_optimization(self, query: str, limit: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Search with quality optimization\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.quality_metrics[\"total_queries\"] += 1\n",
    "        \n",
    "        # Mock retrieval\n",
    "        retrieval_results = [\n",
    "            {\"content\": f\"Result {i} for query: {query}\", \"score\": 0.9 - i * 0.1}\n",
    "            for i in range(limit)\n",
    "        ]\n",
    "        \n",
    "        # Generate initial response\n",
    "        context = \"\\n\".join([r[\"content\"] for r in retrieval_results])\n",
    "        initial_response = f\"Based on the retrieved information: {context[:200]}...\"\n",
    "        \n",
    "        # Assess quality\n",
    "        quality_scores = self.assess_response_quality(query, initial_response, context)\n",
    "        \n",
    "        # Improve response if needed\n",
    "        if quality_scores[\"overall_score\"] < self.quality_threshold:\n",
    "            improved_response = self.improve_response_quality(query, initial_response, context)\n",
    "            self.quality_metrics[\"quality_improvements\"] += 1\n",
    "        else:\n",
    "            improved_response = initial_response\n",
    "        \n",
    "        # Update quality metrics\n",
    "        if quality_scores[\"overall_score\"] >= 0.8:\n",
    "            self.quality_metrics[\"high_quality_responses\"] += 1\n",
    "        elif quality_scores[\"overall_score\"] >= 0.6:\n",
    "            self.quality_metrics[\"medium_quality_responses\"] += 1\n",
    "        else:\n",
    "            self.quality_metrics[\"low_quality_responses\"] += 1\n",
    "        \n",
    "        # Update average quality score\n",
    "        total_responses = self.quality_metrics[\"total_queries\"]\n",
    "        current_avg = self.quality_metrics[\"average_quality_score\"]\n",
    "        self.quality_metrics[\"average_quality_score\"] = (\n",
    "            (current_avg * (total_responses - 1) + quality_scores[\"overall_score\"]) / total_responses\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": improved_response,\n",
    "            \"results\": retrieval_results,\n",
    "            \"quality_scores\": quality_scores,\n",
    "            \"processing_time\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def get_quality_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get quality summary\"\"\"\n",
    "        if self.quality_metrics[\"total_queries\"] == 0:\n",
    "            return {\"message\": \"No quality data available\"}\n",
    "        \n",
    "        total_queries = self.quality_metrics[\"total_queries\"]\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"average_quality_score\": self.quality_metrics[\"average_quality_score\"],\n",
    "            \"high_quality_percentage\": (self.quality_metrics[\"high_quality_responses\"] / total_queries) * 100,\n",
    "            \"medium_quality_percentage\": (self.quality_metrics[\"medium_quality_responses\"] / total_queries) * 100,\n",
    "            \"low_quality_percentage\": (self.quality_metrics[\"low_quality_responses\"] / total_queries) * 100,\n",
    "            \"quality_improvements\": self.quality_metrics[\"quality_improvements\"],\n",
    "            \"improvement_rate\": (self.quality_metrics[\"quality_improvements\"] / total_queries) * 100\n",
    "        }\n",
    "\n",
    "# Test quality-optimized RAG\n",
    "print(\"ðŸ§ª Testing Quality-Optimized RAG System:\")\n",
    "\n",
    "# Create quality-optimized system\n",
    "quality_rag = QualityOptimizedRAG(quality_threshold=0.7)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does artificial intelligence work?\",\n",
    "    \"Explain deep learning algorithms\",\n",
    "    \"What are the applications of data science?\",\n",
    "    \"How do neural networks learn?\"\n",
    "]\n",
    "\n",
    "print(f\"ðŸ” Testing quality optimization with {len(test_queries)} queries:\")\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\nQuery {i+1}: '{query}'\")\n",
    "    \n",
    "    result = quality_rag.search_with_quality_optimization(query, limit=3)\n",
    "    \n",
    "    quality_scores = result[\"quality_scores\"]\n",
    "    print(f\"  Overall quality: {quality_scores['overall_score']:.3f}\")\n",
    "    print(f\"  Relevance: {quality_scores['relevance']:.3f}\")\n",
    "    print(f\"  Completeness: {quality_scores['completeness']:.3f}\")\n",
    "    print(f\"  Accuracy: {quality_scores['accuracy']:.3f}\")\n",
    "    print(f\"  Coherence: {quality_scores['coherence']:.3f}\")\n",
    "    print(f\"  Processing time: {result['processing_time']:.3f}s\")\n",
    "\n",
    "# Get quality summary\n",
    "print(f\"\\nðŸŽ¯ Quality Summary:\")\n",
    "summary = quality_rag.get_quality_summary()\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f405a7",
   "metadata": {},
   "source": [
    "## Caching Strategies {#caching-strategies}\n",
    "\n",
    "Caching is crucial for RAG optimization, reducing latency and costs while improving performance.\n",
    "\n",
    "### Caching Types\n",
    "\n",
    "1. **Query Caching**: Cache complete query responses\n",
    "2. **Embedding Caching**: Cache computed embeddings\n",
    "3. **Result Caching**: Cache retrieval results\n",
    "4. **Context Caching**: Cache processed context\n",
    "5. **Model Caching**: Cache model outputs\n",
    "\n",
    "### Caching Strategies\n",
    "\n",
    "1. **LRU (Least Recently Used)**: Remove least recently used items\n",
    "2. **LFU (Least Frequently Used)**: Remove least frequently used items\n",
    "3. **TTL (Time To Live)**: Remove items after expiration\n",
    "4. **Size-based**: Remove items when cache is full\n",
    "5. **Hybrid**: Combine multiple strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCachingSystem:\n",
    "    \"\"\"Advanced caching system with multiple strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_size: int = 1000,\n",
    "                 ttl: int = 3600,  # 1 hour\n",
    "                 strategy: str = \"lru\"):\n",
    "        \n",
    "        self.max_size = max_size\n",
    "        self.ttl = ttl\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        # Cache storage\n",
    "        self.cache = {}\n",
    "        self.access_times = {}\n",
    "        self.access_counts = {}\n",
    "        self.creation_times = {}\n",
    "        \n",
    "        # Cache metrics\n",
    "        self.metrics = {\n",
    "            \"hits\": 0,\n",
    "            \"misses\": 0,\n",
    "            \"evictions\": 0,\n",
    "            \"total_requests\": 0\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Advanced caching system initialized\")\n",
    "        print(f\"ðŸ“¦ Max size: {max_size}\")\n",
    "        print(f\"â° TTL: {ttl}s\")\n",
    "        print(f\"ðŸ”„ Strategy: {strategy}\")\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"Get value from cache\"\"\"\n",
    "        self.metrics[\"total_requests\"] += 1\n",
    "        \n",
    "        if key not in self.cache:\n",
    "            self.metrics[\"misses\"] += 1\n",
    "            return None\n",
    "        \n",
    "        # Check TTL\n",
    "        if time.time() - self.creation_times[key] > self.ttl:\n",
    "            self._evict(key)\n",
    "            self.metrics[\"misses\"] += 1\n",
    "            return None\n",
    "        \n",
    "        # Update access information\n",
    "        self.access_times[key] = time.time()\n",
    "        self.access_counts[key] = self.access_counts.get(key, 0) + 1\n",
    "        \n",
    "        self.metrics[\"hits\"] += 1\n",
    "        return self.cache[key]\n",
    "    \n",
    "    def set(self, key: str, value: Any) -> None:\n",
    "        \"\"\"Set value in cache\"\"\"\n",
    "        # Check if cache is full\n",
    "        if len(self.cache) >= self.max_size and key not in self.cache:\n",
    "            self._evict_oldest()\n",
    "        \n",
    "        # Store value\n",
    "        self.cache[key] = value\n",
    "        self.access_times[key] = time.time()\n",
    "        self.access_counts[key] = 1\n",
    "        self.creation_times[key] = time.time()\n",
    "    \n",
    "    def _evict_oldest(self) -> None:\n",
    "        \"\"\"Evict oldest item based on strategy\"\"\"\n",
    "        if not self.cache:\n",
    "            return\n",
    "        \n",
    "        if self.strategy == \"lru\":\n",
    "            # Remove least recently used\n",
    "            oldest_key = min(self.access_times, key=self.access_times.get)\n",
    "        elif self.strategy == \"lfu\":\n",
    "            # Remove least frequently used\n",
    "            oldest_key = min(self.access_counts, key=self.access_counts.get)\n",
    "        else:\n",
    "            # Default to LRU\n",
    "            oldest_key = min(self.access_times, key=self.access_times.get)\n",
    "        \n",
    "        self._evict(oldest_key)\n",
    "    \n",
    "    def _evict(self, key: str) -> None:\n",
    "        \"\"\"Evict specific key\"\"\"\n",
    "        if key in self.cache:\n",
    "            del self.cache[key]\n",
    "            del self.access_times[key]\n",
    "            del self.access_counts[key]\n",
    "            del self.creation_times[key]\n",
    "            self.metrics[\"evictions\"] += 1\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache metrics\"\"\"\n",
    "        total_requests = self.metrics[\"total_requests\"]\n",
    "        hit_rate = self.metrics[\"hits\"] / total_requests if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_requests\": total_requests,\n",
    "            \"hits\": self.metrics[\"hits\"],\n",
    "            \"misses\": self.metrics[\"misses\"],\n",
    "            \"evictions\": self.metrics[\"evictions\"],\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"max_size\": self.max_size,\n",
    "            \"utilization\": len(self.cache) / self.max_size\n",
    "        }\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        self.access_times.clear()\n",
    "        self.access_counts.clear()\n",
    "        self.creation_times.clear()\n",
    "        print(\"âœ… Cache cleared\")\n",
    "\n",
    "# Test caching system\n",
    "print(\"ðŸ§ª Testing Advanced Caching System:\")\n",
    "\n",
    "# Create caching system\n",
    "cache = AdvancedCachingSystem(max_size=5, ttl=60, strategy=\"lru\")\n",
    "\n",
    "# Test caching\n",
    "test_data = [\n",
    "    (\"key1\", \"value1\"),\n",
    "    (\"key2\", \"value2\"),\n",
    "    (\"key3\", \"value3\"),\n",
    "    (\"key4\", \"value4\"),\n",
    "    (\"key5\", \"value5\"),\n",
    "    (\"key6\", \"value6\"),  # This should evict key1\n",
    "    (\"key7\", \"value7\"),  # This should evict key2\n",
    "]\n",
    "\n",
    "print(\"ðŸ“ Adding data to cache:\")\n",
    "for key, value in test_data:\n",
    "    cache.set(key, value)\n",
    "    print(f\"  Set {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸ” Testing cache retrieval:\")\n",
    "for key in [\"key1\", \"key2\", \"key3\", \"key4\", \"key5\", \"key6\", \"key7\"]:\n",
    "    value = cache.get(key)\n",
    "    if value:\n",
    "        print(f\"  Hit: {key} = {value}\")\n",
    "    else:\n",
    "        print(f\"  Miss: {key}\")\n",
    "\n",
    "# Get cache metrics\n",
    "print(f\"\\nðŸ“Š Cache Metrics:\")\n",
    "metrics = cache.get_metrics()\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test TTL\n",
    "print(f\"\\nâ° Testing TTL (waiting 2 seconds):\")\n",
    "time.sleep(2)\n",
    "cache.set(\"test_key\", \"test_value\")\n",
    "print(f\"  Set test_key with TTL\")\n",
    "time.sleep(2)\n",
    "value = cache.get(\"test_key\")\n",
    "if value:\n",
    "    print(f\"  Hit: test_key = {value}\")\n",
    "else:\n",
    "    print(f\"  Miss: test_key (expired)\")\n",
    "\n",
    "# Clear cache\n",
    "cache.clear()\n",
    "print(f\"\\nâœ… Cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72dce5",
   "metadata": {},
   "source": [
    "## Key Takeaways & Next Steps\n",
    "\n",
    "### What We've Optimized\n",
    "âœ… **Performance Optimization** with batch processing and efficient algorithms\n",
    "âœ… **Cost Optimization** with caching and token management\n",
    "âœ… **Quality Optimization** with response assessment and improvement\n",
    "âœ… **Advanced Caching** with multiple strategies and TTL\n",
    "âœ… **Monitoring & Metrics** for continuous improvement\n",
    "\n",
    "### Key Insights\n",
    "1. **Performance vs Quality**: Balance speed with accuracy\n",
    "2. **Cost vs Performance**: Optimize for your specific needs\n",
    "3. **Caching is Critical**: Reduces costs and improves performance\n",
    "4. **Quality Monitoring**: Essential for maintaining user satisfaction\n",
    "5. **Continuous Optimization**: Regular monitoring and improvement needed\n",
    "\n",
    "### Next Steps\n",
    "- **Production Deployment**: Implement in production environment\n",
    "- **A/B Testing**: Test different optimization strategies\n",
    "- **User Feedback**: Integrate user feedback for quality improvement\n",
    "- **Advanced Monitoring**: Implement comprehensive monitoring\n",
    "- **Cost Analysis**: Regular cost analysis and optimization\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "- **Federated Caching**: Distributed caching systems\n",
    "- **Predictive Caching**: ML-based cache optimization\n",
    "- **Quality Learning**: Adaptive quality improvement\n",
    "- **Cost Prediction**: Predictive cost modeling\n",
    "- **Performance Tuning**: Advanced performance optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to optimize your RAG system?** Start with performance monitoring, then gradually add caching, cost optimization, and quality improvement based on your specific requirements!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
