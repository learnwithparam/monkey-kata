{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3ac899",
   "metadata": {},
   "source": [
    "# RAG Fundamentals: Building Your First Retrieval-Augmented Generation System\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to RAG](#introduction)\n",
    "2. [Core Components](#core-components)\n",
    "3. [Document Processing Pipeline](#document-processing)\n",
    "4. [Embedding Generation](#embedding-generation)\n",
    "5. [Vector Storage & Retrieval](#vector-storage)\n",
    "6. [Query Processing](#query-processing)\n",
    "7. [Response Generation](#response-generation)\n",
    "8. [Complete RAG Implementation](#complete-implementation)\n",
    "9. [Evaluation & Metrics](#evaluation)\n",
    "10. [Real-World Applications](#applications)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to RAG {#introduction}\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of information retrieval and text generation. Instead of relying solely on the LLM's training data, RAG allows us to:\n",
    "\n",
    "- **Retrieve** relevant information from a knowledge base\n",
    "- **Augment** the LLM's context with this information\n",
    "- **Generate** responses based on both the query and retrieved context\n",
    "\n",
    "### Why RAG Matters\n",
    "\n",
    "**Problem**: LLMs have knowledge cutoff dates and can't access real-time information\n",
    "**Solution**: RAG provides access to up-to-date, domain-specific knowledge\n",
    "**Result**: More accurate, factual, and contextually relevant responses\n",
    "\n",
    "### Business Impact\n",
    "- **Customer Support**: 60% reduction in ticket volume\n",
    "- **Legal Analysis**: 20+ hours/week saved in document review\n",
    "- **Knowledge Management**: 95% accuracy in Q&A systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecf044",
   "metadata": {},
   "source": [
    "## Core Components {#core-components}\n",
    "\n",
    "A RAG system consists of several key components working together:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Documents] --> B[Document Processor]\n",
    "    B --> C[Text Chunker]\n",
    "    C --> D[Embedding Model]\n",
    "    D --> E[Vector Database]\n",
    "    F[User Query] --> G[Query Processor]\n",
    "    G --> H[Embedding Model]\n",
    "    H --> I[Vector Search]\n",
    "    E --> I\n",
    "    I --> J[Retrieved Context]\n",
    "    J --> K[LLM]\n",
    "    F --> K\n",
    "    K --> L[Generated Response]\n",
    "```\n",
    "\n",
    "### 1. Document Processing\n",
    "- **Input**: Raw documents (PDFs, text files, web pages)\n",
    "- **Output**: Cleaned, structured text chunks\n",
    "- **Key Tasks**: Text extraction, cleaning, chunking\n",
    "\n",
    "### 2. Embedding Generation\n",
    "- **Purpose**: Convert text to numerical vectors\n",
    "- **Models**: Sentence Transformers, OpenAI embeddings\n",
    "- **Output**: High-dimensional vectors representing semantic meaning\n",
    "\n",
    "### 3. Vector Storage\n",
    "- **Purpose**: Store and index embeddings for fast retrieval\n",
    "- **Options**: Qdrant, Chroma, Pinecone, Weaviate\n",
    "- **Features**: Similarity search, filtering, metadata\n",
    "\n",
    "### 4. Query Processing\n",
    "- **Input**: User question\n",
    "- **Process**: Convert to embedding, search similar vectors\n",
    "- **Output**: Relevant document chunks\n",
    "\n",
    "### 5. Response Generation\n",
    "- **Input**: Query + Retrieved context\n",
    "- **Process**: LLM generates response using both\n",
    "- **Output**: Contextual, accurate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sentence-transformers qdrant-client redis python-dotenv tiktoken\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import redis\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# Import our LLM provider utilities\n",
    "import sys\n",
    "sys.path.append('../../utils')\n",
    "from llm_providers import get_available_providers, LLMProviderFactory, LLMConfig\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM provider (will use first available provider)\n",
    "available_providers = get_available_providers()\n",
    "if not available_providers:\n",
    "    raise ValueError(\"No LLM providers available! Please check your .env file and API keys.\")\n",
    "\n",
    "# Use the first available provider\n",
    "provider_name = list(available_providers.keys())[0]\n",
    "llm_provider = available_providers[provider_name]\n",
    "\n",
    "print(f\"âœ… All packages imported successfully!\")\n",
    "print(f\"ðŸ”§ Using LLM provider: {provider_name.upper()}\")\n",
    "print(f\"ðŸŒ Available providers: {list(available_providers.keys())}\")\n",
    "print(\"ðŸ”§ Environment configured for RAG implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752a05c",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline {#document-processing}\n",
    "\n",
    "The first step in any RAG system is processing raw documents into a format suitable for embedding and retrieval.\n",
    "\n",
    "### Sample Knowledge Base: E-commerce Product Catalog\n",
    "\n",
    "Let's work with a realistic e-commerce scenario - building a product recommendation and Q&A system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad575c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample e-commerce product data\n",
    "sample_products = [\n",
    "    {\n",
    "        \"id\": \"laptop-001\",\n",
    "        \"name\": \"MacBook Pro 16-inch M3 Max\",\n",
    "        \"category\": \"Laptops\",\n",
    "        \"price\": 3999.99,\n",
    "        \"description\": \"Powerful laptop with M3 Max chip, 32GB RAM, 1TB SSD. Perfect for professional video editing, 3D rendering, and software development. Features Liquid Retina XDR display with ProMotion technology.\",\n",
    "        \"specs\": {\n",
    "            \"processor\": \"Apple M3 Max\",\n",
    "            \"memory\": \"32GB unified memory\",\n",
    "            \"storage\": \"1TB SSD\",\n",
    "            \"display\": \"16.2-inch Liquid Retina XDR\",\n",
    "            \"graphics\": \"38-core GPU\",\n",
    "            \"battery\": \"Up to 22 hours\",\n",
    "            \"ports\": \"3x Thunderbolt 4, HDMI, SDXC, MagSafe 3\"\n",
    "        },\n",
    "        \"features\": [\"Professional performance\", \"Long battery life\", \"High-resolution display\", \"Silent operation\"],\n",
    "        \"target_audience\": \"Professional developers, video editors, graphic designers\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"laptop-002\", \n",
    "        \"name\": \"Dell XPS 13 Plus\",\n",
    "        \"category\": \"Laptops\",\n",
    "        \"price\": 1299.99,\n",
    "        \"description\": \"Ultra-thin laptop with 13th Gen Intel Core i7, 16GB RAM, 512GB SSD. Premium design with edge-to-edge keyboard and haptic touchpad. Ideal for business professionals and content creators.\",\n",
    "        \"specs\": {\n",
    "            \"processor\": \"Intel Core i7-1360P\",\n",
    "            \"memory\": \"16GB LPDDR5\",\n",
    "            \"storage\": \"512GB PCIe SSD\",\n",
    "            \"display\": \"13.4-inch 3.5K OLED\",\n",
    "            \"graphics\": \"Intel Iris Xe\",\n",
    "            \"battery\": \"Up to 12 hours\",\n",
    "            \"ports\": \"2x Thunderbolt 4, 1x USB-C\"\n",
    "        },\n",
    "        \"features\": [\"Ultra-thin design\", \"OLED display\", \"Premium build quality\", \"Fast performance\"],\n",
    "        \"target_audience\": \"Business professionals, content creators, students\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"phone-001\",\n",
    "        \"name\": \"iPhone 15 Pro Max\",\n",
    "        \"category\": \"Smartphones\", \n",
    "        \"price\": 1199.99,\n",
    "        \"description\": \"Latest iPhone with A17 Pro chip, 256GB storage, titanium design. Features advanced camera system with 5x optical zoom, Action Button, and USB-C connectivity.\",\n",
    "        \"specs\": {\n",
    "            \"processor\": \"A17 Pro chip\",\n",
    "            \"memory\": \"8GB RAM\",\n",
    "            \"storage\": \"256GB\",\n",
    "            \"display\": \"6.7-inch Super Retina XDR\",\n",
    "            \"camera\": \"48MP main, 12MP ultra-wide, 12MP telephoto\",\n",
    "            \"battery\": \"Up to 29 hours video playback\",\n",
    "            \"connectivity\": \"5G, Wi-Fi 6E, Bluetooth 5.3\"\n",
    "        },\n",
    "        \"features\": [\"Titanium design\", \"Advanced camera system\", \"USB-C\", \"Action Button\"],\n",
    "        \"target_audience\": \"Photography enthusiasts, power users, professionals\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"headphone-001\",\n",
    "        \"name\": \"Sony WH-1000XM5\",\n",
    "        \"category\": \"Audio\",\n",
    "        \"price\": 399.99,\n",
    "        \"description\": \"Industry-leading noise-canceling headphones with 30-hour battery life. Features LDAC codec, speak-to-chat technology, and premium comfort for all-day listening.\",\n",
    "        \"specs\": {\n",
    "            \"driver\": \"30mm dynamic\",\n",
    "            \"frequency\": \"4Hz-40kHz\",\n",
    "            \"battery\": \"30 hours (NC on), 40 hours (NC off)\",\n",
    "            \"charging\": \"3 min = 3 hours playback\",\n",
    "            \"connectivity\": \"Bluetooth 5.2, NFC, 3.5mm jack\",\n",
    "            \"weight\": \"250g\",\n",
    "            \"features\": [\"Industry-leading ANC\", \"LDAC codec\", \"Speak-to-chat\", \"Quick attention mode\"]\n",
    "        },\n",
    "        \"features\": [\"Industry-leading noise cancellation\", \"30-hour battery\", \"Premium sound quality\", \"Comfortable design\"],\n",
    "        \"target_audience\": \"Audiophiles, frequent travelers, remote workers\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“¦ Loaded {len(sample_products)} sample products\")\n",
    "print(\"ðŸŽ¯ Categories:\", set(product[\"category\"] for product in sample_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Represents a chunk of text with metadata\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: List[float] = None\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document processing and chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def create_product_document(self, product: Dict[str, Any]) -> str:\n",
    "        \"\"\"Convert product data to a searchable document\"\"\"\n",
    "        doc_parts = [\n",
    "            f\"Product: {product['name']}\",\n",
    "            f\"Category: {product['category']}\",\n",
    "            f\"Price: ${product['price']}\",\n",
    "            f\"Description: {product['description']}\",\n",
    "            f\"Target Audience: {product['target_audience']}\",\n",
    "            f\"Key Features: {', '.join(product['features'])}\",\n",
    "            f\"Specifications: {', '.join([f'{k}: {v}' for k, v in product['specs'].items()])}\"\n",
    "        ]\n",
    "        return \"\\n\".join(doc_parts)\n",
    "    \n",
    "    def chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[DocumentChunk]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        \n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            # Get chunk of words\n",
    "            end = min(start + self.chunk_size, len(words))\n",
    "            chunk_words = words[start:end]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "            \n",
    "            # Create chunk ID\n",
    "            chunk_id_str = f\"{metadata.get('product_id', 'unknown')}_chunk_{chunk_id}\"\n",
    "            \n",
    "            # Create chunk\n",
    "            chunk = DocumentChunk(\n",
    "                id=chunk_id_str,\n",
    "                content=chunk_text,\n",
    "                metadata=metadata.copy()\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # Move start position with overlap\n",
    "            start = end - self.chunk_overlap\n",
    "            chunk_id += 1\n",
    "            \n",
    "            # Prevent infinite loop\n",
    "            if start >= len(words) - self.chunk_overlap:\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_products(self, products: List[Dict[str, Any]]) -> List[DocumentChunk]:\n",
    "        \"\"\"Process all products into chunks\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for product in products:\n",
    "            # Create document\n",
    "            doc = self.create_product_document(product)\n",
    "            \n",
    "            # Create metadata\n",
    "            metadata = {\n",
    "                \"product_id\": product[\"id\"],\n",
    "                \"product_name\": product[\"name\"],\n",
    "                \"category\": product[\"category\"],\n",
    "                \"price\": product[\"price\"],\n",
    "                \"source\": \"ecommerce_catalog\"\n",
    "            }\n",
    "            \n",
    "            # Chunk document\n",
    "            chunks = self.chunk_text(doc, metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "# Initialize processor\n",
    "processor = DocumentProcessor(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Process products into chunks\n",
    "chunks = processor.process_products(sample_products)\n",
    "\n",
    "print(f\"ðŸ“„ Created {len(chunks)} document chunks\")\n",
    "print(f\"ðŸ“Š Average chunk length: {np.mean([len(chunk.content) for chunk in chunks]):.0f} characters\")\n",
    "print(f\"ðŸ” Sample chunk:\")\n",
    "print(f\"ID: {chunks[0].id}\")\n",
    "print(f\"Content: {chunks[0].content[:200]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826709b",
   "metadata": {},
   "source": [
    "## Embedding Generation {#embedding-generation}\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar texts will have similar embeddings, enabling semantic search.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Dimensionality**: Higher dimensions = more detail but more storage/compute\n",
    "- **Model Choice**: Different models excel at different tasks\n",
    "- **Normalization**: Ensures consistent vector magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de467213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles text embedding generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize embedding model\n",
    "        \n",
    "        Popular models:\n",
    "        - all-MiniLM-L6-v2: Fast, good for general purpose (384 dims)\n",
    "        - all-mpnet-base-v2: Better quality, slower (768 dims)\n",
    "        - sentence-transformers/all-MiniLM-L12-v2: Balanced (384 dims)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        print(f\"âœ… Loaded embedding model: {model_name}\")\n",
    "        print(f\"ðŸ“ Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    def generate_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a single text\"\"\"\n",
    "        embedding = self.model.encode(text, convert_to_tensor=False)\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for multiple texts efficiently\"\"\"\n",
    "        embeddings = self.model.encode(texts, convert_to_tensor=False)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def normalize_embedding(self, embedding: List[float]) -> List[float]:\n",
    "        \"\"\"Normalize embedding to unit vector\"\"\"\n",
    "        embedding_array = np.array(embedding)\n",
    "        norm = np.linalg.norm(embedding_array)\n",
    "        if norm == 0:\n",
    "            return embedding\n",
    "        return (embedding_array / norm).tolist()\n",
    "\n",
    "# Initialize embedding generator\n",
    "embedder = EmbeddingGenerator(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(\"ðŸ”„ Generating embeddings for all chunks...\")\n",
    "chunk_texts = [chunk.content for chunk in chunks]\n",
    "embeddings = embedder.generate_embeddings_batch(chunk_texts)\n",
    "\n",
    "# Add embeddings to chunks\n",
    "for chunk, embedding in zip(chunks, embeddings):\n",
    "    chunk.embedding = embedder.normalize_embedding(embedding)\n",
    "\n",
    "print(f\"âœ… Generated {len(embeddings)} embeddings\")\n",
    "print(f\"ðŸ“Š Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"ðŸ” Sample embedding (first 5 values): {embeddings[0][:5]}\")\n",
    "\n",
    "# Test semantic similarity\n",
    "test_queries = [\n",
    "    \"laptop for video editing\",\n",
    "    \"wireless headphones with noise cancellation\", \n",
    "    \"expensive smartphone with good camera\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ§ª Testing semantic similarity:\")\n",
    "for query in test_queries:\n",
    "    query_embedding = embedder.normalize_embedding(embedder.generate_embedding(query))\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for chunk in chunks:\n",
    "        similarity = np.dot(query_embedding, chunk.embedding)\n",
    "        similarities.append((similarity, chunk.metadata['product_name'], chunk.content[:100]))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(reverse=True)\n",
    "    \n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"Top matches:\")\n",
    "    for i, (sim, name, content) in enumerate(similarities[:2]):\n",
    "        print(f\"  {i+1}. {name} (similarity: {sim:.3f})\")\n",
    "        print(f\"     {content}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8e968",
   "metadata": {},
   "source": [
    "## Vector Storage & Retrieval {#vector-storage}\n",
    "\n",
    "Vector databases enable fast similarity search over large collections of embeddings. We'll use Qdrant for this example.\n",
    "\n",
    "### Vector Database Features:\n",
    "- **Similarity Search**: Find most similar vectors quickly\n",
    "- **Filtering**: Combine vector search with metadata filters\n",
    "- **Scalability**: Handle millions of vectors efficiently\n",
    "- **Persistence**: Store vectors permanently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Handles vector storage and retrieval using Qdrant\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"ecommerce_products\"):\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Get Qdrant configuration from environment\n",
    "        qdrant_url = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "        qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "        \n",
    "        # Initialize Qdrant client\n",
    "        if qdrant_api_key:\n",
    "            self.client = QdrantClient(\n",
    "                url=qdrant_url,\n",
    "                api_key=qdrant_api_key\n",
    "            )\n",
    "        else:\n",
    "            # For development, use in-memory client\n",
    "            self.client = QdrantClient(\":memory:\")\n",
    "            print(\"âš ï¸  Using in-memory Qdrant client. Data will not persist between sessions.\")\n",
    "            print(\"   To use persistent storage, set QDRANT_URL and QDRANT_API_KEY in your .env file\")\n",
    "        \n",
    "        self.embedding_dim = 384  # all-MiniLM-L6-v2 dimension\n",
    "        \n",
    "    def create_collection(self):\n",
    "        \"\"\"Create a new collection in the vector database\"\"\"\n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.embedding_dim,\n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"âœ… Created collection: {self.collection_name}\")\n",
    "        except Exception as e:\n",
    "            # Collection might already exist\n",
    "            print(f\"â„¹ï¸  Collection {self.collection_name} already exists or error: {e}\")\n",
    "    \n",
    "    def add_documents(self, chunks: List[DocumentChunk]):\n",
    "        \"\"\"Add document chunks to the vector database\"\"\"\n",
    "        points = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            point = PointStruct(\n",
    "                id=hash(chunk.id) % (2**63),  # Convert to int64\n",
    "                vector=chunk.embedding,\n",
    "                payload={\n",
    "                    \"content\": chunk.content,\n",
    "                    \"metadata\": chunk.metadata,\n",
    "                    \"chunk_id\": chunk.id\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch insert points\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        print(f\"âœ… Added {len(points)} documents to vector store\")\n",
    "    \n",
    "    def search(self, query_embedding: List[float], limit: int = 5, \n",
    "               filter_conditions: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        search_result = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit,\n",
    "            query_filter=filter_conditions\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for hit in search_result:\n",
    "            results.append({\n",
    "                \"score\": hit.score,\n",
    "                \"content\": hit.payload[\"content\"],\n",
    "                \"metadata\": hit.payload[\"metadata\"],\n",
    "                \"chunk_id\": hit.payload[\"chunk_id\"]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Get information about the collection\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(self.collection_name)\n",
    "            return {\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"indexed_vectors_count\": info.indexed_vectors_count,\n",
    "                \"points_count\": info.points_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Could not get collection info: {e}\"}\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# Create collection\n",
    "vector_store.create_collection()\n",
    "\n",
    "# Add all chunks to vector store\n",
    "vector_store.add_documents(chunks)\n",
    "\n",
    "# Get collection info\n",
    "info = vector_store.get_collection_info()\n",
    "print(f\"ðŸ“Š Collection info: {info}\")\n",
    "\n",
    "# Test search functionality\n",
    "print(\"\\nðŸ” Testing vector search:\")\n",
    "test_query = \"laptop for professional video editing\"\n",
    "query_embedding = embedder.generate_embedding(test_query)\n",
    "\n",
    "results = vector_store.search(query_embedding, limit=3)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(\"Top results:\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n{i+1}. Score: {result['score']:.3f}\")\n",
    "    print(f\"   Product: {result['metadata']['product_name']}\")\n",
    "    print(f\"   Category: {result['metadata']['category']}\")\n",
    "    print(f\"   Content: {result['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db4c1c",
   "metadata": {},
   "source": [
    "## Query Processing {#query-processing}\n",
    "\n",
    "The query processing component handles user questions and retrieves relevant context. This is where we implement search strategies and result ranking.\n",
    "\n",
    "### Query Processing Strategies:\n",
    "1. **Direct Search**: Use query as-is\n",
    "2. **Query Expansion**: Add synonyms and related terms\n",
    "3. **Query Rewriting**: Restructure for better retrieval\n",
    "4. **Hybrid Search**: Combine vector and keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    \"\"\"Handles query processing and retrieval strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedder: EmbeddingGenerator):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedder = embedder\n",
    "        \n",
    "        # Query expansion patterns for e-commerce\n",
    "        self.expansion_patterns = {\n",
    "            \"laptop\": [\"notebook\", \"computer\", \"PC\", \"workstation\"],\n",
    "            \"phone\": [\"smartphone\", \"mobile\", \"device\"],\n",
    "            \"headphone\": [\"headset\", \"earphone\", \"audio\", \"headphones\"],\n",
    "            \"cheap\": [\"affordable\", \"budget\", \"inexpensive\", \"low-cost\"],\n",
    "            \"expensive\": [\"premium\", \"high-end\", \"luxury\", \"costly\"],\n",
    "            \"good\": [\"excellent\", \"great\", \"quality\", \"reliable\"],\n",
    "            \"fast\": [\"quick\", \"rapid\", \"speedy\", \"high-performance\"]\n",
    "        }\n",
    "    \n",
    "    def expand_query(self, query: str) -> str:\n",
    "        \"\"\"Expand query with synonyms and related terms\"\"\"\n",
    "        words = query.lower().split()\n",
    "        expanded_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            expanded_words.append(word)\n",
    "            if word in self.expansion_patterns:\n",
    "                expanded_words.extend(self.expansion_patterns[word][:2])  # Add top 2 synonyms\n",
    "        \n",
    "        return \" \".join(expanded_words)\n",
    "    \n",
    "    def process_query(self, query: str, strategy: str = \"semantic\", \n",
    "                     limit: int = 5, filters: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process user query and retrieve relevant documents\n",
    "        \n",
    "        Strategies:\n",
    "        - semantic: Use semantic similarity only\n",
    "        - expanded: Use query expansion\n",
    "        - hybrid: Combine semantic + keyword matching\n",
    "        \"\"\"\n",
    "        \n",
    "        if strategy == \"expanded\":\n",
    "            expanded_query = self.expand_query(query)\n",
    "            print(f\"ðŸ” Expanded query: '{expanded_query}'\")\n",
    "            query_embedding = self.embedder.generate_embedding(expanded_query)\n",
    "        else:\n",
    "            query_embedding = self.embedder.generate_embedding(query)\n",
    "        \n",
    "        # Retrieve documents\n",
    "        results = self.vector_store.search(\n",
    "            query_embedding=query_embedding,\n",
    "            limit=limit,\n",
    "            filter_conditions=filters\n",
    "        )\n",
    "        \n",
    "        # Post-process results\n",
    "        processed_results = []\n",
    "        for result in results:\n",
    "            processed_result = {\n",
    "                \"content\": result[\"content\"],\n",
    "                \"metadata\": result[\"metadata\"],\n",
    "                \"relevance_score\": result[\"score\"],\n",
    "                \"chunk_id\": result[\"chunk_id\"]\n",
    "            }\n",
    "            processed_results.append(processed_result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    def get_context_for_llm(self, query: str, max_chunks: int = 3) -> str:\n",
    "        \"\"\"Get formatted context for LLM input\"\"\"\n",
    "        results = self.process_query(query, limit=max_chunks)\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            context_parts.append(\n",
    "                f\"Document {i}:\\n\"\n",
    "                f\"Product: {result['metadata']['product_name']}\\n\"\n",
    "                f\"Category: {result['metadata']['category']}\\n\"\n",
    "                f\"Content: {result['content']}\\n\"\n",
    "                f\"Relevance: {result['relevance_score']:.3f}\\n\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "\n",
    "# Initialize query processor\n",
    "query_processor = QueryProcessor(vector_store, embedder)\n",
    "\n",
    "# Test different query strategies\n",
    "test_queries = [\n",
    "    \"What's the best laptop for video editing?\",\n",
    "    \"I need cheap wireless headphones\",\n",
    "    \"Show me expensive phones with good cameras\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing query processing strategies:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    \n",
    "    # Test semantic search\n",
    "    print(\"\\nðŸ“Š Semantic Search Results:\")\n",
    "    results = query_processor.process_query(query, strategy=\"semantic\", limit=2)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  {i}. {result['metadata']['product_name']} (score: {result['relevance_score']:.3f})\")\n",
    "    \n",
    "    # Test expanded search\n",
    "    print(\"\\nðŸ” Expanded Search Results:\")\n",
    "    results = query_processor.process_query(query, strategy=\"expanded\", limit=2)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  {i}. {result['metadata']['product_name']} (score: {result['relevance_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc068a84",
   "metadata": {},
   "source": [
    "## Response Generation {#response-generation}\n",
    "\n",
    "The final step combines the user query with retrieved context to generate a comprehensive, accurate response using an LLM.\n",
    "\n",
    "### Response Generation Best Practices:\n",
    "1. **Clear Context**: Provide relevant background information\n",
    "2. **Structured Output**: Use consistent formatting\n",
    "3. **Source Attribution**: Reference where information came from\n",
    "4. **Confidence Indication**: Show certainty levels when appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGenerator:\n",
    "    \"\"\"Handles response generation using LLM provider abstraction\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider=None):\n",
    "        self.llm_provider = llm_provider or llm_provider\n",
    "        \n",
    "    def create_system_prompt(self) -> str:\n",
    "        \"\"\"Create system prompt for the LLM\"\"\"\n",
    "        return \"\"\"You are an expert e-commerce assistant helping customers find the right products. \n",
    "\n",
    "Your role:\n",
    "- Analyze product information and customer queries\n",
    "- Provide accurate, helpful recommendations\n",
    "- Be specific about product features and benefits\n",
    "- Always base your answers on the provided context\n",
    "- If information is not available in the context, say so clearly\n",
    "\n",
    "Guidelines:\n",
    "- Be conversational but professional\n",
    "- Include specific product names and key features\n",
    "- Mention prices when relevant\n",
    "- Explain why a product might be suitable\n",
    "- Suggest alternatives when appropriate\"\"\"\n",
    "\n",
    "    def create_user_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create user prompt with query and context\"\"\"\n",
    "        return f\"\"\"Customer Query: {query}\n",
    "\n",
    "Relevant Product Information:\n",
    "{context}\n",
    "\n",
    "Please provide a helpful response based on the product information above. Include specific product recommendations with reasoning.\"\"\"\n",
    "\n",
    "    def generate_response(self, query: str, context: str, \n",
    "                         temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate response using LLM provider\"\"\"\n",
    "        try:\n",
    "            # Create the full prompt\n",
    "            system_prompt = self.create_system_prompt()\n",
    "            user_prompt = self.create_user_prompt(query, context)\n",
    "            full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "            \n",
    "            # Generate using the provider\n",
    "            result = self.llm_provider.generate(\n",
    "                full_prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            # Extract response based on provider type\n",
    "            if 'error' in result:\n",
    "                return f\"Error generating response: {result['error']}\"\n",
    "            \n",
    "            # Handle different response formats\n",
    "            if 'choices' in result and len(result['choices']) > 0:\n",
    "                return result['choices'][0]['message']['content'].strip()\n",
    "            elif 'candidates' in result and len(result['candidates']) > 0:\n",
    "                return result['candidates'][0]['content']['parts'][0]['text'].strip()\n",
    "            elif 'response' in result:\n",
    "                return result['response'].strip()\n",
    "            else:\n",
    "                return str(result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def generate_structured_response(self, query: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate structured response with metadata\"\"\"\n",
    "        response_text = self.generate_response(query, context)\n",
    "        \n",
    "        # Extract product mentions (simple regex-based approach)\n",
    "        import re\n",
    "        product_pattern = r'([A-Z][a-zA-Z\\s]+(?:Pro|Max|Plus|XPS|MacBook|iPhone|Sony))'\n",
    "        mentioned_products = re.findall(product_pattern, response_text)\n",
    "        \n",
    "        return {\n",
    "            \"response\": response_text,\n",
    "            \"mentioned_products\": list(set(mentioned_products)),\n",
    "            \"query\": query,\n",
    "            \"context_length\": len(context),\n",
    "            \"provider\": self.llm_provider.config.provider,\n",
    "            \"model\": self.llm_provider.config.model\n",
    "        }\n",
    "\n",
    "# Initialize response generator\n",
    "response_generator = ResponseGenerator(llm_provider)\n",
    "\n",
    "# Test response generation\n",
    "test_queries = [\n",
    "    \"What's the best laptop for video editing under $2000?\",\n",
    "    \"I need wireless headphones with good noise cancellation\",\n",
    "    \"Show me the most expensive phone with the best camera\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ¤– Testing response generation:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Get context\n",
    "    context = query_processor.get_context_for_llm(query, max_chunks=3)\n",
    "    \n",
    "    # Generate response\n",
    "    structured_response = response_generator.generate_structured_response(query, context)\n",
    "    \n",
    "    print(f\"\\nResponse:\")\n",
    "    print(structured_response[\"response\"])\n",
    "    print(f\"\\nMentioned Products: {structured_response['mentioned_products']}\")\n",
    "    print(f\"Context Length: {structured_response['context_length']} characters\")\n",
    "    print(f\"Provider: {structured_response['provider']} ({structured_response['model']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2f964",
   "metadata": {},
   "source": [
    "## Complete RAG Implementation {#complete-implementation}\n",
    "\n",
    "Now let's put it all together into a complete RAG system that can be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf70428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system integrating all components with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 llm_provider=None,\n",
    "                 chunk_size: int = 300,\n",
    "                 chunk_overlap: int = 50):\n",
    "        \n",
    "        # Initialize components\n",
    "        self.embedder = EmbeddingGenerator(embedding_model)\n",
    "        self.processor = DocumentProcessor(chunk_size, chunk_overlap)\n",
    "        self.vector_store = VectorStore()\n",
    "        self.query_processor = QueryProcessor(self.vector_store, self.embedder)\n",
    "        self.response_generator = ResponseGenerator(llm_provider)\n",
    "        \n",
    "        # Initialize Redis cache\n",
    "        self.redis_client = self._init_redis()\n",
    "        \n",
    "        # Create collection\n",
    "        self.vector_store.create_collection()\n",
    "        \n",
    "        print(\"ðŸš€ RAG System initialized successfully!\")\n",
    "    \n",
    "    def _init_redis(self):\n",
    "        \"\"\"Initialize Redis client with fallback\"\"\"\n",
    "        try:\n",
    "            redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "            client = redis.from_url(redis_url, decode_responses=True)\n",
    "            # Test connection\n",
    "            client.ping()\n",
    "            print(\"âœ… Redis cache connected\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Redis not available: {e}\")\n",
    "            print(\"   Caching disabled - using in-memory fallback\")\n",
    "            return None\n",
    "    \n",
    "    def _get_cache_key(self, query: str, strategy: str, max_chunks: int) -> str:\n",
    "        \"\"\"Generate cache key for query\"\"\"\n",
    "        import hashlib\n",
    "        key_data = f\"{query}_{strategy}_{max_chunks}\"\n",
    "        return f\"rag:{hashlib.md5(key_data.encode()).hexdigest()}\"\n",
    "    \n",
    "    def _get_from_cache(self, cache_key: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get response from cache\"\"\"\n",
    "        if not self.redis_client:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            cached = self.redis_client.get(cache_key)\n",
    "            if cached:\n",
    "                return json.loads(cached)\n",
    "        except Exception as e:\n",
    "            print(f\"Cache read error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _set_cache(self, cache_key: str, response: Dict[str, Any], ttl: int = 3600):\n",
    "        \"\"\"Store response in cache\"\"\"\n",
    "        if not self.redis_client:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.redis_client.setex(cache_key, ttl, json.dumps(response))\n",
    "        except Exception as e:\n",
    "            print(f\"Cache write error: {e}\")\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"Add documents to the knowledge base\"\"\"\n",
    "        print(f\"ðŸ“„ Processing {len(documents)} documents...\")\n",
    "        \n",
    "        # Process documents into chunks\n",
    "        chunks = self.processor.process_products(documents)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(\"ðŸ”„ Generating embeddings...\")\n",
    "        chunk_texts = [chunk.content for chunk in chunks]\n",
    "        embeddings = self.embedder.generate_embeddings_batch(chunk_texts)\n",
    "        \n",
    "        # Add embeddings to chunks\n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            chunk.embedding = self.embedder.normalize_embedding(embedding)\n",
    "        \n",
    "        # Store in vector database\n",
    "        self.vector_store.add_documents(chunks)\n",
    "        \n",
    "        print(f\"âœ… Added {len(chunks)} chunks to knowledge base\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question: str, \n",
    "              strategy: str = \"semantic\",\n",
    "              max_chunks: int = 3,\n",
    "              temperature: float = 0.7,\n",
    "              use_cache: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system with caching\"\"\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache:\n",
    "            cache_key = self._get_cache_key(question, strategy, max_chunks)\n",
    "            cached_response = self._get_from_cache(cache_key)\n",
    "            if cached_response:\n",
    "                print(f\"ðŸŽ¯ Cache hit for query: '{question}'\")\n",
    "                cached_response[\"cached\"] = True\n",
    "                return cached_response\n",
    "        \n",
    "        print(f\"ðŸ” Processing query: '{question}'\")\n",
    "        \n",
    "        # Get relevant context\n",
    "        context = self.query_processor.get_context_for_llm(\n",
    "            question, max_chunks=max_chunks\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        structured_response = self.response_generator.generate_structured_response(\n",
    "            question, context\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        structured_response.update({\n",
    "            \"strategy\": strategy,\n",
    "            \"max_chunks\": max_chunks,\n",
    "            \"temperature\": temperature,\n",
    "            \"timestamp\": str(pd.Timestamp.now()),\n",
    "            \"cached\": False\n",
    "        })\n",
    "        \n",
    "        # Cache the response\n",
    "        if use_cache:\n",
    "            self._set_cache(cache_key, structured_response)\n",
    "        \n",
    "        return structured_response\n",
    "    \n",
    "    def get_system_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system statistics\"\"\"\n",
    "        collection_info = self.vector_store.get_collection_info()\n",
    "        cache_stats = {}\n",
    "        \n",
    "        if self.redis_client:\n",
    "            try:\n",
    "                cache_info = self.redis_client.info('memory')\n",
    "                cache_stats = {\n",
    "                    \"redis_connected\": True,\n",
    "                    \"redis_memory_used\": cache_info.get('used_memory_human', 'Unknown'),\n",
    "                    \"redis_keys\": self.redis_client.dbsize()\n",
    "                }\n",
    "            except:\n",
    "                cache_stats = {\"redis_connected\": False}\n",
    "        else:\n",
    "            cache_stats = {\"redis_connected\": False}\n",
    "        \n",
    "        return {\n",
    "            \"total_documents\": collection_info[\"points_count\"],\n",
    "            \"embedding_model\": self.embedder.model_name,\n",
    "            \"llm_provider\": self.response_generator.llm_provider.config.provider,\n",
    "            \"llm_model\": self.response_generator.llm_provider.config.model,\n",
    "            \"collection_name\": self.vector_store.collection_name,\n",
    "            **cache_stats\n",
    "        }\n",
    "\n",
    "# Initialize complete RAG system\n",
    "rag_system = RAGSystem(llm_provider=llm_provider)\n",
    "\n",
    "# Add our sample products\n",
    "rag_system.add_documents(sample_products)\n",
    "\n",
    "# Get system stats\n",
    "stats = rag_system.get_system_stats()\n",
    "print(f\"\\nðŸ“Š System Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test the complete system\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ðŸ§ª Testing Complete RAG System\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "test_questions = [\n",
    "    \"What's the best laptop for professional video editing?\",\n",
    "    \"I need wireless headphones under $500 with excellent noise cancellation\",\n",
    "    \"Show me the most expensive smartphone with the best camera system\",\n",
    "    \"What laptop would be good for a student on a budget?\",\n",
    "    \"I want a phone with long battery life and good performance\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    response = rag_system.query(question, strategy=\"semantic\", max_chunks=2)\n",
    "    \n",
    "    print(f\"\\nAnswer:\")\n",
    "    print(response[\"response\"])\n",
    "    print(f\"\\nMentioned Products: {response['mentioned_products']}\")\n",
    "    print(f\"Context Length: {response['context_length']} characters\")\n",
    "    print(f\"Cached: {response.get('cached', False)}\")\n",
    "    print(f\"Provider: {response.get('llm_provider', 'Unknown')} ({response.get('llm_model', 'Unknown')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97265366",
   "metadata": {},
   "source": [
    "## Evaluation & Metrics {#evaluation}\n",
    "\n",
    "Evaluating RAG systems is crucial for ensuring quality and performance. Let's implement some basic evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaa8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluates RAG system performance\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "    def evaluate_retrieval_quality(self, queries: List[str], \n",
    "                                 expected_products: List[List[str]]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate retrieval quality using precision and recall\"\"\"\n",
    "        \n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for query, expected in zip(queries, expected_products):\n",
    "            # Get retrieval results\n",
    "            results = self.rag_system.query_processor.process_query(query, limit=3)\n",
    "            retrieved_products = [r['metadata']['product_name'] for r in results]\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            if retrieved_products:\n",
    "                precision = len(set(retrieved_products) & set(expected)) / len(retrieved_products)\n",
    "                precisions.append(precision)\n",
    "            else:\n",
    "                precisions.append(0.0)\n",
    "            \n",
    "            if expected:\n",
    "                recall = len(set(retrieved_products) & set(expected)) / len(expected)\n",
    "                recalls.append(recall)\n",
    "            else:\n",
    "                recalls.append(0.0)\n",
    "        \n",
    "        return {\n",
    "            \"precision\": np.mean(precisions),\n",
    "            \"recall\": np.mean(recalls),\n",
    "            \"f1_score\": 2 * np.mean(precisions) * np.mean(recalls) / (np.mean(precisions) + np.mean(recalls)) if (np.mean(precisions) + np.mean(recalls)) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def evaluate_response_time(self, queries: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate response time for queries\"\"\"\n",
    "        \n",
    "        response_times = []\n",
    "        \n",
    "        for query in queries:\n",
    "            start_time = time.time()\n",
    "            self.rag_system.query(query)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            response_times.append(end_time - start_time)\n",
    "        \n",
    "        return {\n",
    "            \"avg_response_time\": np.mean(response_times),\n",
    "            \"min_response_time\": np.min(response_times),\n",
    "            \"max_response_time\": np.max(response_times),\n",
    "            \"std_response_time\": np.std(response_times)\n",
    "        }\n",
    "    \n",
    "    def evaluate_context_relevance(self, queries: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate how relevant the retrieved context is\"\"\"\n",
    "        \n",
    "        relevance_scores = []\n",
    "        \n",
    "        for query in queries:\n",
    "            # Get context\n",
    "            context = self.rag_system.query_processor.get_context_for_llm(query, max_chunks=3)\n",
    "            \n",
    "            # Simple relevance check: count query terms in context\n",
    "            query_terms = set(query.lower().split())\n",
    "            context_terms = set(context.lower().split())\n",
    "            \n",
    "            if query_terms:\n",
    "                relevance = len(query_terms & context_terms) / len(query_terms)\n",
    "                relevance_scores.append(relevance)\n",
    "        \n",
    "        return {\n",
    "            \"avg_relevance\": np.mean(relevance_scores),\n",
    "            \"min_relevance\": np.min(relevance_scores),\n",
    "            \"max_relevance\": np.max(relevance_scores)\n",
    "        }\n",
    "    \n",
    "    def comprehensive_evaluation(self, test_queries: List[str], \n",
    "                               expected_products: List[List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive evaluation\"\"\"\n",
    "        \n",
    "        print(\"ðŸ” Running comprehensive RAG evaluation...\")\n",
    "        \n",
    "        # Retrieval quality\n",
    "        retrieval_metrics = self.evaluate_retrieval_quality(test_queries, expected_products)\n",
    "        \n",
    "        # Response time\n",
    "        response_time_metrics = self.evaluate_response_time(test_queries)\n",
    "        \n",
    "        # Context relevance\n",
    "        relevance_metrics = self.evaluate_context_relevance(test_queries)\n",
    "        \n",
    "        # Combine all metrics\n",
    "        evaluation_results = {\n",
    "            \"retrieval_quality\": retrieval_metrics,\n",
    "            \"response_time\": response_time_metrics,\n",
    "            \"context_relevance\": relevance_metrics,\n",
    "            \"overall_score\": (\n",
    "                retrieval_metrics[\"f1_score\"] * 0.4 +\n",
    "                (1 - min(response_time_metrics[\"avg_response_time\"] / 5, 1)) * 0.3 +\n",
    "                relevance_metrics[\"avg_relevance\"] * 0.3\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "# Test evaluation\n",
    "evaluator = RAGEvaluator(rag_system)\n",
    "\n",
    "# Test queries with expected products\n",
    "test_queries = [\n",
    "    \"What's the best laptop for video editing?\",\n",
    "    \"I need wireless headphones with noise cancellation\",\n",
    "    \"Show me expensive smartphones with good cameras\"\n",
    "]\n",
    "\n",
    "expected_products = [\n",
    "    [\"MacBook Pro 16-inch M3 Max\"],  # Expected for video editing\n",
    "    [\"Sony WH-1000XM5\"],  # Expected for noise cancellation\n",
    "    [\"iPhone 15 Pro Max\"]  # Expected for expensive phone with good camera\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluator.comprehensive_evaluation(test_queries, expected_products)\n",
    "\n",
    "print(\"\\nðŸ“Š Evaluation Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Retrieval Quality:\")\n",
    "for metric, value in evaluation_results[\"retrieval_quality\"].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nâ±ï¸ Response Time:\")\n",
    "for metric, value in evaluation_results[\"response_time\"].items():\n",
    "    print(f\"  {metric}: {value:.3f}s\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Context Relevance:\")\n",
    "for metric, value in evaluation_results[\"context_relevance\"].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ† Overall Score: {evaluation_results['overall_score']:.3f}\")\n",
    "\n",
    "# Performance interpretation\n",
    "overall_score = evaluation_results['overall_score']\n",
    "if overall_score >= 0.8:\n",
    "    performance_level = \"Excellent\"\n",
    "elif overall_score >= 0.6:\n",
    "    performance_level = \"Good\"\n",
    "elif overall_score >= 0.4:\n",
    "    performance_level = \"Fair\"\n",
    "else:\n",
    "    performance_level = \"Needs Improvement\"\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Performance Level: {performance_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca8dc6",
   "metadata": {},
   "source": [
    "## Real-World Applications {#applications}\n",
    "\n",
    "RAG systems are being used across various industries to solve real business problems. Here are some examples:\n",
    "\n",
    "### 1. Customer Support (60% ticket reduction)\n",
    "- **Problem**: High volume of repetitive customer questions\n",
    "- **Solution**: RAG-powered Q&A system with knowledge base\n",
    "- **Result**: Faster response times, reduced agent workload\n",
    "\n",
    "### 2. Legal Document Analysis (20+ hours/week saved)\n",
    "- **Problem**: Manual review of contracts and legal documents\n",
    "- **Solution**: RAG system trained on legal corpus\n",
    "- **Result**: Automated clause extraction and risk assessment\n",
    "\n",
    "### 3. E-commerce Product Recommendations\n",
    "- **Problem**: Customers can't find relevant products\n",
    "- **Solution**: RAG system with product catalog and customer queries\n",
    "- **Result**: Improved product discovery and conversion rates\n",
    "\n",
    "### 4. Technical Documentation Q&A\n",
    "- **Problem**: Developers struggle to find relevant documentation\n",
    "- **Solution**: RAG system with code examples and documentation\n",
    "- **Result**: Faster onboarding and problem resolution\n",
    "\n",
    "### 5. Healthcare Knowledge Base\n",
    "- **Problem**: Medical professionals need quick access to latest research\n",
    "- **Solution**: RAG system with medical literature and guidelines\n",
    "- **Result**: Better patient care and treatment decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76c6d4",
   "metadata": {},
   "source": [
    "## Key Takeaways & Next Steps\n",
    "\n",
    "### What We've Built\n",
    "âœ… **Complete RAG System** with document processing, embedding generation, vector storage, and response generation\n",
    "âœ… **Evaluation Framework** for measuring system performance\n",
    "âœ… **Real-world Example** using e-commerce product recommendations\n",
    "\n",
    "### Key Concepts Learned\n",
    "1. **Document Processing**: Chunking strategies and metadata management\n",
    "2. **Embeddings**: Converting text to numerical vectors for semantic search\n",
    "3. **Vector Storage**: Efficient storage and retrieval of embeddings\n",
    "4. **Query Processing**: Different strategies for handling user queries\n",
    "5. **Response Generation**: Combining context with LLM for accurate answers\n",
    "\n",
    "### Next Steps\n",
    "- **Advanced RAG**: Hybrid search, reranking, and query expansion\n",
    "- **Production Deployment**: Scaling, monitoring, and optimization\n",
    "- **Evaluation**: More sophisticated metrics and testing frameworks\n",
    "- **Specialized Applications**: Domain-specific RAG implementations\n",
    "\n",
    "### Performance Optimization Tips\n",
    "1. **Chunk Size**: Experiment with different sizes (200-500 tokens)\n",
    "2. **Embedding Models**: Choose based on your domain and language\n",
    "3. **Vector Database**: Consider scalability and feature requirements\n",
    "4. **Query Processing**: Implement query expansion and rewriting\n",
    "5. **Caching**: Cache frequent queries and embeddings\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to build your own RAG system?** Start with a small dataset, implement the basic components, and gradually add advanced features based on your specific use case!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
